
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Starter Template for Bootstrap</title>

    <!-- Bootstrap core CSS -->
    <link href="dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

    <!-- Custom styles for this template -->
    <link href="starter-template.css" rel="stylesheet">

    <!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
    <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->
    <!-- <script src="../../assets/js/ie-emulation-modes-warning.js"></script> -->

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>

  <body>

    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="#">Sparky - Robot Companion</a>
        </div>
        <div id="navbar" class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li class="active"><a href="#">Home</a></li>
            <li><a href="#obj">Project Objective</a></li>
            <li><a href="#intro">Introduction</a></li>
            <li><a href="#design">Design and Testing</a></li>
            <li><a href="#result">Results</a></li>
            <li><a href="#conclusion">Conclusion</a></li>
            <li><a href="#futurework">Future Work</a></li>
            <li><a href="#team">Team</a></li>
            <li><a href="#apendix">Apendixes</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

    <div class="container">

      <div class="starter-template">
        <h1>Sparky - Robot Companion</h1>
        </p><p class="c51 subtitle" id="h.hpmrc74rl0fj">
	<p class="c20 c49 c5">Monday Section</p>
        </p><p class="c51 subtitle" id="h.hpmrc74rl0fj-1">
	<p class="c21"><p class="c5">ECE 5725 Final Project</h1>
	</p><p class="c51 subtitle" id="h.hpmrc74rl0fj-2">
	<p class="c20 c49 c5">By: John Ly (jtl22) and Carlos Gutierrez (cag334)</h1>
      </div>x

      <hr>
      <div class="center-block">
          <iframe width="640" height="360" src="https://www.youtube.com/embed/yTBZ8xb8o1w" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"allowfullscreen></iframe>
          <h4 style="text-align:center;">Demonstration Video</h4>
      </div>

      <hr id="obj ">

      <div style="text-align:center;">
              <h2>Project Objective:</h2>
	      </h1><p class="c3"><span class="c2">
	The goal for our ECE 5725: Design with Embedded Operating Systems final
        project was to create a smart mobile robot which uniquely combined OpenCV facial recognition alongside
	voice commands to create a fun robot capable of identifying faces and traveling to them upon voice requests.</span>
      </div>

    <hr id='intro'>

      <div style="text-align:center;">
          <h1 class="c46" id="h.vc2avdc0nzd7">
		<span class="c31 c20">Introduction</span>
	  </h1>
	<p class="c3"><span>Our final project is named Sparky. Sparky is an intelligent pet-like robot which listens to its 
		owner and is able to accomplish unique tricks in some ways similar to a real live trained dog. The basis for 
		this project is the Raspberry Pi 4 system combined with a piTFT display, a Raspberry Pi Camera, a microphone, 
		and a motorized chassis. Using both </span><span class="c19">
		<a class="c17" href="https://www.google.com/url?q=https://docs.opencv.org/master/index.html&amp;sa=D&amp;ust=1608452692603000&amp;usg=AOvVaw1b__CZFwsKjHQBZn78LL2g">OpenCV</a></span><span>&nbsp;and the Python </span><span class="c19"><a class="c17" href="https://www.google.com/url?q=https://github.com/ageitgey/face_recognition&amp;sa=D&amp;ust=1608452692603000&amp;usg=AOvVaw262bU8aGsOaLb-OJ-TZNxa">
		face_recognition</a></span><span>&nbsp;library, we are able to display on-screen video feedback of our camera 
		signal with additional image processing to detect faces. A connected microphone allows Sparky to actively record 
		sounds and listen for key words to act upon before doing a desired action, to do so Sparky uses the </span>
		<span class="c19"><a class="c17" href="https://www.google.com/url?q=https://github.com/Picovoice/picovoice&amp;sa=D&amp;ust=1608452692604000&amp;usg=AOvVaw3-pqobr9FeJur0Vvsh4bLt">
			picovoice</a></span><span>&nbsp;</span><span>library along with custom created files using </span>
		<span class="c19"><a class="c17" href="https://www.google.com/url?q=https://picovoice.ai/docs/quick-start/console-rhino/&amp;sa=D&amp;ust=1608452692604000&amp;usg=AOvVaw3moY7PoDSCXuNnMavxd84k">
			picovoice</a></span><span class="c19"><a class="c17" href="https://www.google.com/url?q=https://picovoice.ai/docs/quick-start/console-rhino/&amp;sa=D&amp;ust=1608452692604000&amp;usg=AOvVaw3moY7PoDSCXuNnMavxd84k">&nbsp;console</a>
		</span><span class="c2">. Lastly, the Raspberry Pi 4 is seated up above an acrylic frame in addition to a rechargeable 
		battery pack, 4 AA batteries for our two DC motors, the motor controller, and our two wheels for locomotion.</span>
	      </p>
	      
	      <p class="c3 c7"><span class="c2"></span></p>
        	<p class="c21">
            <span style="overflow: hidden; display: inline-block; margin: 0px 0px; border: 0px solid #000000; transform: rotate(0rad) translateZ(0px); -webkit-transform: rotate(0rad) translateZ(0px); width: 245px; height: 265.16px;">
                <img alt="" src="pics/image4.png" style="width: 304.7px; height: 303.48px; margin-left: -25.26px; margin-top: -38.31px; transform: rotate(0rad) translateZ(0px); -webkit-transform: rotate(0rad) translateZ(0px);" title="" />
            </span>
            <span style="overflow: hidden; display: inline-block; margin: 0px 0px; border: 0px solid #000000; transform: rotate(0rad) translateZ(0px); -webkit-transform: rotate(0rad) translateZ(0px); width: 210.26px; height: 268.5px;">
                <img alt="" src="pics/image7.png" style="width: 210.26px; height: 268.5px; margin-left: 0px; margin-top: 0px; transform: rotate(0rad) translateZ(0px); -webkit-transform: rotate(0rad) translateZ(0px);" title="" />
            </span>
        </p>
        <p class="c21"><span class="c2">Figure 1: Sparky Build - John&rsquo;s on left (mic not shown), Carlos&rsquo;s on right </span></p>
	      

	      
          <img class="img-rounded" src="pics/1.jpg" alt="Generic placeholder image" width="240" height="240">
          </div>
          <div class="col-md-8" style="font-size:18px;">
          <h2>Project Objective:</h2>
          <ul>
              <li>some important objectives.some important objectives.some important objectives.some important objectives.</li>
                <li>some other important objectives.</li>
            <li>some not-that-important objectives.</li>
          </ul>
          </div>
      </div>

    <hr id='design'>

      <div style="text-align:center;">
              <h2>Design</h2>
              <p style="text-align: left;padding: 0px 30px;">
                <h1>Design and Testing</h1>


                <h2>Build and Construction</h2>
                
                
                <p>
                
                
                <p id="gdcalert5" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image5.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert6">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>
                
                
                <img src="images/image5.png" width="" alt="alt_text" title="image_tooltip">
                
                </p>
                <p>
                Figure 2: Sparky Wiring and Circuit Design
                </p>
                <p>
                The assembly of our robo pet builds off of a previous lab, Lab 3, reusing the same chassis, motors, motor controller, and battery design. The major additions are the camera, the microphone, and additional construction materials for mounting.
                </p>
                <p>
                This project uses a Raspberry Pi 4 2GB with PiTFT Display on top. Connected to the display is a breakout cable which is inserted into a full sized breadboard. Here on this breadboard, we have wiring for our motor controller as well as our two DC motors, and Sparky’s notification LED. These objects are all attached to a thin acrylic plate with mounting holes. While our overall construction is not ideal in terms of sturdiness, it is perfectly suitable for prototyping our design. Underneath and towards the back of our chassis is a ~2200mAh lithium ion battery responsible for directly powering our Raspberry Pi when Sparky is set to work untethered. Towards the front of the chassis is a 6V AA battery holder used to power our motors. Our motor controller is the Sparkfun TB6612FNG dual-channel motor controller as shown below. It gives us control of both independent motor operation as well as duty cycle to assist with making turns and changing speeds.
                </p>
                <p>
                <em>PWMA 	    → 	GPIO 26		PWMB 	    → 	GPIO 16</em>
                </p>
                <p>
                <em>INA1 	    →  	GPIO 4			INB1 	    →  	GPIO 20</em>
                </p>
                <p>
                <em>INA2	    →  	GPIO 5			INB2 	    →  	GPIO 21</em>
                </p>
                <p>
                
                
                <p id="gdcalert6" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image6.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert7">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>
                
                
                <img src="images/image6.png" width="" alt="alt_text" title="image_tooltip">
                
                </p>
                <p>
                Sparkfun TB6612FNG dual-channel motor controller
                </p>
                <p>
                Shown below is the underside with our battery placement, motors, and wheels all easily visible. 
                </p>
                <p>
                
                
                <p id="gdcalert7" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image7.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert8">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>
                
                
                <img src="images/image7.png" width="" alt="alt_text" title="image_tooltip">
                
                </p>
                <p>
                Figure 4: Underside of Sparky
                </p>
                <p>
                As seen in <strong>Figure 1</strong>, the Raspberry Pi V2 Camera Module is placed directly facing forward and as high as possible upon the robot to get an angle closer to head level for face detection and recognition. Due to the constraints of our build, Sparky performs most optimally on a table surface where one’s face is most visible from the camera’s perspective due to the height. In a more advanced build there would be more room for an actuated camera with an additional degree of freedom to pivot and look around during operation. Our cameras are stood up onto our robot primarily using cardboard/wood splints. What is not displayed here is that Sparky additionally uses a USB connected mic for receiving audio. We individually found our own mics to use, but any small and lightweight omnidirectional microphone is ideal here. John substituted in a Blue Snowball iCE Microphone with a long wire while Carlos used a small headset and strapped it around Sparky.
                </p>
                <h2>Voice recognition</h2>
                
                
                <h3>Microphone Input:</h3>
                
                
                <p>
                With any USB microphone connected to the Py we use the pyaudio() library to establish an audio stream from the microphone by setting variable <code>pa = pyaudio.PyAudio(). </code>Next we open the audio stream<code> pa.open()</code> and set the sample rate, channels, format, and size of the buffer inside this variable (configuration based on the pyadudio library). The values used for the <code>rate</code>,<code> format</code> and <code>frames_per_buffer</code> are taken from the picovoice library because we will later process the audio stream using the picovoice library and there must be congruence between the audio format and the necessary requirements of the picovoice library. The audio_stream variable setup is shown below. 
                </p>
                
                
                
                <pre class="prettyprint">audio_stream = pa.open(
                  rate=_picovoice.sample_rate,
                  channels=1,
                  format=pyaudio.paInt16,
                  input=True,
                  frames_per_buffer=_picovoice.frame_length)</pre>
                
                
                <h3><strong>Using Picovoice():</strong> </h3>
                
                
                <p>
                The picovoice engine accepts 16-bit linearly-encoded PCM and operates on a single-channel. The audio is processed in consecutive frames. Picovoice is able to understand wake words (hotword) and after the recognition of the wake word it detects the intentions of the user using inference. Both the wake word and the inference are customizable and can be created using the online picovoice console, after the desired phrases are created and trained online we can download the trained files that pertain to the wake word and inferences. The wake word is .ppn type file and the inference is a .rhn type file. The path to these files are placed inside variable <code>_picovoice</code> along with other configurations seen below.
                </p>
                
                
                
                <pre class="prettyprint">_picovoice=Picovoice(
                  keyword_path=_keyword_path,
                  wake_word_callback=wake_word_callback,
                  context_path=_context_path,
                  inference_callback=inference_callback,
                  porcupine_library_path=None,
                  porcupine_model_path=None,
                  rhino_sensitivity=0.2, 
                  rhino_library_path=None,
                  rhino_model_path=None,
                  porcupine_sensitivity=1)</pre>
                
                
                <p>
                Variables<code> wake_word_callback</code> and<code> inferecence_callback</code> are assigned to a respective function that handles the signal of such an event occurring. Variables <code>rhino_sensitivity</code> and<code> porcupine_sensitivy</code> are assigned a value from [0-1] where 1 is the most sensitive value for the library to react to inferences and to the wake word respectively. We set the hot word sensitivity to the 1 to make sure a call to Sparky will be very easily recognized. The value set for the inference is low to make sure that the spoken commands are correctly ottered and so that the library is not set off by incorrectly uttered commands  that are not part of Sparky’s understanding. 
                </p>
                <p>
                We opted for our audio to be constantly running and processing the information gathered from the microphone, therefore at the beginning of the while loop we set variable<code> pcm = audio_stream.read(_picovoice.frame_length,exception_on_overflow=False)</code> in order to read the microphone, on the next line we edit the variable <code>pcm = struct.unpack_from("h" * _picovoice.frame_length, pcm) </code>to unpack the information from the audio into a format picovoice expects and finally we process the data<code> _picovoice.process(pcm)</code>.
                </p>
                <p>
                <em>def wake_word_callback():</em>
                </p>
                <p>
                This function is accessed when the word “Sparky” is recognized by picovoice engine. Here we set all mode flags to false, light up the LED connected to GPIO pin 13, set flag intention.led_on to True which will allow us to turn off the LED after 5 seconds, we also stop the motors of the robot. The main purpose behind this is to give the sense that when Sparky is called he stops what he is doing and listens to the next instructions. 
                </p>
                <p>
                <em>def inference_callback():</em>
                </p>
                <p>
                After the wake word is recognized and the inferences spoken are understood this function accessed in here we enable the flags that set off the modes inside the while based on the values of<code> inference.intent</code> which can be either ‘move’ or search’ strings. Variable <code>inference. slots[] </code>is a dictionary type and contains the value of slot under the uttered intent. Along with enabling the correct flag we write the value of the action Sparky is taking into variable <code>intention.word</code> as a string ( used later on to write on the TFT display). We initiate variable <code>timerStart= time.time()</code> to be used in modes where a time limit is implemented, we read this value to know when the mode began. 
                </p>
                <h2>Face Detection and Recognition</h2>
                
                
                <h3>Face Detection Algorithm</h3>
                
                
                <p>
                Overall while many face detection and recognition libraries, choosing an appropriate method for our device had some challenges. Initially, we developed Sparky using the Local Binary Patterns Histograms (LBPH) algorithm available through the OpenCV library. More information about how this method works can be found <a href="https://towardsdatascience.com/face-recognition-how-lbph-works-90ec258c3d6b">here</a>, but in summary the algorithm takes in grayscale images and uses thresholding to turn the pixel array into binary data. From here a histogram of the data is created and based on new incoming images. Those images are treated similarly and compared across various histogram values. Based on some differences between the values, a confidence value can be calculated to determine a person’s face. Initially this method seemed to work fine, properly identifying and labeling John’s face, but after further testing, we found that while the algorithm works okay for already pretrained images, it had some biases towards already trained faces and resulted in lots of errors especially with faces that were supposed to be unknown to the system. 
                </p>
                <p>
                As a result, towards the later half of the project we pivoted towards using the significantly more accurate face_recognition library linked <a href="https://github.com/ageitgey/face_recognition">here</a>. According to the documentation, this library was trained using the <a href="http://dlib.net/">Dlib C++</a> library along with deep learning methods. The result here is that key facial features are determined primarily through color gradient changes within a face. These results are recorded in a histogram of oriented gradients (HOG) which is then encoded for usage with our facial recognition model. With these gradients it is possible to discern key landmark features such as the eye shape, eyebrows, nose shape, mouth, and jawline. Additional details on the deep learning algorithm used can be found <a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78">here</a>.
                </p>
                <p>
                In our pivot from using the LBPH model method provided in OpenCV to the face_recogniton library written by ageitgey, there were a few major resulting changes. Firstly, face recognition was significantly improved and with very high accuracy. The major tradeoff though was that the necessary computing power was much more significant and resulted in our smooth video feed losing many frames. A few factors are the cause for this, but some potential solutions for improving computation times would be having a processor better suited for calculated parallel computations and with better single core performance. On a standard desktop, this can be achieved through using Nvidia’s CUDA cores or a high performance CPU. One potential other issue is that our compilation of dlib using pip3 install may have some strange performance errors as opposed to compiling the library on board our Raspberry Pi. This has not been tested though as this potential solution was only recently found by us.
                </p>
                <h3>Face Recognition Training Data</h3>
                
                
                <p>
                As Sparky is currently, he only can recognize pretrained faces. It would very much be possible to have active face recognition work with new faces during operation but we’ll leave that for a possible future implementation. Using this <a href="https://www.tomshardware.com/how-to/raspberry-pi-facial-recognition">guide</a> from Caroline Dunn, we were able to make a face recognition system that ran using active video data instead of just identifying still images as provided by the face_recognition library. To first train our encodings for face recognition, we start by taking a collection of photos of primarily one's face while rotated to different angles. For John, he additionally took photos with and without glasses. We used about 20-30 images but the face_detection library is supposedly very capable with even just 10 images. With our input data of identifying names and images, we first take our image data captured in BGR format and convert it to RGB before using the face recognition library to determine face positions as well as create encodings in HOG model format.
                </p>
                
                
                
                <pre class="prettyprint">    boxes = face_recognition.face_locations(rgb, model="hog")
                    encodings = face_recognition.face_encodings(rgb, boxes)
                    for encoding in encodings:
                        knownEncodings.append(encoding)
                        knownNames.append(name)
                </pre>
                
                
                <p>
                Lastly, using the Python pickle library allows us to save our data into a serialized format.
                </p>
                
                
                
                <pre class="prettyprint">data = {"encodings": knownEncodings, "names": knownNames}
                f = open("encodings.pickle", "wb")
                f.write(pickle.dumps(data))
                f.close()
                </pre>
                
                
                <h3>Face Recognition Implementation</h3>
                
                
                <p>
                For both face recognition algorithms described, our system first required face detection using OpenCV’s Haar Feature-based Cascade Classifier model which is able to quickly detect objects, but in our case faces. The difference between face detection and face recognition is that recognition is more specific and allows for us to distinguish between faces whereas face detection only provides us with the ability to determine if a human face is present. The model is available pre-trained but is generally created using lots of positive (faces) and negative (no faces) images. From there, the differences allow for key features to be extracted and to create a model. The return from this face detection when using <code>detector.detectMultiScale() </code>where detector is a <code>CascadeClassifier</code> is a set of 4 values describing the pixel values of a box surrounding the detected person’s face from within an image. The format of this is (x,y,w,h) where x and y describe the starting position of the box from the top left corner while w and h describe the pixel width and height of the box respectively. While this classifier only uses grayscale images, our face recognition algorithm uses RGB format images thus we must create both forms of images. For those unfamiliar, it is also important to note that in pixel arrays (0,0) starts from the top left corner.
                </p>
                
                
                
                <pre class="prettyprint">    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
                    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                
                    # simple face detection
                    faces = detector.detectMultiScale( img_gray, scaleFactor = 1.3, 
                minNeighbors = 5, minSize = (int(minW), int(minH)), 
                flags = cv2.CASCADE_SCALE_IMAGE
                    )
                </pre>
                
                
                <p>
                Using this data we are able to zoom in on the data that we want to process for face recognition and individually compare each boxed face to our encoded HOG dataset for identification. While the function <code>detect_faces_quick() </code>primarily just uses the above lines of code for face detection, the function <code>identify_faces() </code>is used for proper face recognition and deals with the comparisons as previously mentioned. Using new camera data and previously trained encodings created from the face_recogntion library, we can make our identifications and label the faces accordingly. 
                </p>
                <p>
                In addition to the OpenCV library being responsible for opening up our camera and reading the incoming signal, we also used it for drawing and labeling on top of our images. The library includes the ability to draw shapes and write text as desired at different pixel coordinates. Some of that can be seen below. In the case that we cannot identify a person, the written name is defaulted to “Unknown” and they are labeled as such.
                </p>
                
                
                
                <pre class="prettyprint">  cv2.ellipse(img, (center[0], center[1]), (w//2, h//2), 0, 0, 360, (255, 0, 255), 2)   
                  cv2.putText(img, str(name), (leftX-15,bottomY+30), font, 1.2, (255,255,255), 2)
                </pre>
                
                
                <h3>Move mode:</h3>
                
                
                <p>
                When <code>instruction.v_direction</code> is True, Sparky’s motors move in the direction specified by the user that direction is read from variable instruction.word. For each of the directions there is a call to function <code>movement_3sec("direction",speed, speed)</code> which will make the motors move in the “direction” uttered. Inside the function<code> movement_3sec()</code> a timer is implemented for directions ('forward', 'backward', 'right', and  'left') of half a second implemented to make Sparky stop after the timer is over. Using pygame we also update the TFT display message to display what Sparky is doing. 
                </p>
                
                
                
                <pre class="prettyprint">if instruction.v_direction:
                  if instruction.word == 'forward':
                        movement_3sec('forward',90,90)
                  if instruction.word == 'back':
                        movement_3sec('backward',90,90)
                  if instruction.word == 'right':
                        movement_3sec('right',90,90)
                  if instruction.word == 'left':
                        movement_3sec('left',90,90)
                  if instruction.word == 'spin':
                        tw.drive("left",90,90)
                      
                  screen.fill(BLACK, textAreaRect)
                  text = fontLg.render(f'Moving {instruction.word}', True, WHITE, BLACK)
                  textRect = text.get_rect(center=(0.5*dispW, 0.75*dispH))
                  screen.fill(BLACK, textRect.inflate(240,0))
                            screen.blit(text,textRect)
                            pygame.display.update(textRect)</pre>
                
                
                <h3>Search Mode:</h3>
                
                
                <p>
                When variable <code>instruction.v_search</code> is true we can tell Sparky to begin the process of finding the user specified. First the text on theTFT is updated to show the change in mode, Sparky displays ‘Looking for ‘user’...’. A timer condition of 20 seconds is set so that Sparky only performs the task of searching for 20 seconds. Inside this condition there is a call to the function <code>foundFace = find_faces(instruction.word, img)</code>. If the function returns True, that means the target user was found in that direction, then we stop running the<code> find_faces</code> function and we proceed to running<strong> </strong>the <code>detect_faces_quick</code> function. Our return value,<code> facePresent = detect_faces_quick(img)</code>will be true if there is a singular detectable face still in the view of the camera. If facePresent is true we perform <code>pursue_target(True)</code> which will guide the robot closer towards the face based on the location of the face on the image from the camera. If <code>facePresent</code> is false then we run the function <code>identify_faces()</code> which determines if the user requested is found in the field of view of the camera, this function then returns a value for <code>target</code> and then again we perform<code> pursue_target</code> if<code> target=True.</code>In the case of<code> target= False</code> we increment variable<code> misses</code>, after 10 consecutive misses (<code>misses >10</code>) we then try again and run the <code>find_faces</code> function. 
                </p>
                <p>
                The main reason for dividing the face recognition steps like this is to make an adjustment between running a very heavy face recognition process, which provides the best results on identifying the user, and being able to pursuing the target in the correct direction. If the heavy face recognition function is always running it has an enormous effect on the framerate of the camera which then translates to erroneous movements of the robot because we are moving faster than what our image processing can return. This causes a delay on the reaction of the motors and a bad performance by Sparky. We opted to implement the search algorithm this way so that after finding our target our driving motion towards the face consists of using face detection only for approximately 95% of the time (light on the processor) and then we use proper face recognition for the remaining other 5% of the time (heavy on the processor). This way we make sure that Sparky makes faster motor adjustment decisions when moving towards the target and also that he recognizes the face with high precision. Once Sparky reaches the target user, the flag<code> stopCondition</code> is set and we exit the search mode (set<code> instruction.v_search =False</code>) and set all variables back to their starting value, we also update the TFT display to notify that the user was found. While Sparky is trying to find the user, a counter is displayed onto the screen showing how much time has elapsed since the search began, and as previously stated once the count reaches its limit of 20 seconds, Sparky stops the search.
                </p>
                <h3>Tricks Mode:</h3>
                
                
                <p>
                For Sparky to go into this mode the variabel<code> instruction.v_tricks</code> is set to true by the voice recognition. Inside this condition we look at variable<code> instruction.word</code> and determine if it's equal to ‘party’ or ‘break dance’, the two tricks that Sparky can do. 
                </p>
                <p>
                If <code>instruction.word == 'party', </code>we then again update the tft display to show what Sparky is doing and we run function <code>_,_,img = identify_faces(None, img, mode = "party_time")</code> this is the same face detection function previously used by the search mode but since in this case the motors for Sparky are not running we take the privilege to run it without constraints. We pass mode= “party_time” to the function to tell it that it will not only do face recognition but will also draw on top of the user the trade-marked party hats. For users that Sparky knows (Carlos and John) the hat is special and for people that Sparky doesn’t know yet they will receive just an outlined triangle. This mode will run for 10 seconds and also display a 10 second counter on the screen at the end of the time Sparky exits the mode and waits for his next instructions. 
                </p>
                <p>
                
                
                <p id="gdcalert8" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image8.gif). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert9">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>
                
                
                <img src="images/image8.gif" width="" alt="alt_text" title="image_tooltip">
                
                
                <p id="gdcalert9" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image9.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert10">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>
                
                
                <img src="images/image9.png" width="" alt="alt_text" title="image_tooltip">
                
                </p>
                <p>
                If <code>instruction.word == 'break dance'</code>, then TFT display is updated and a function call to move_breakdance() is made. This function moves the motors so that sparky alternates between moving right and left. Sparky looks like he is wiggling in place. 
                </p>
                <p>
                
                
                <p id="gdcalert10" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image10.gif). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert11">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>
                
                
                <img src="images/image10.gif" width="" alt="alt_text" title="image_tooltip">
                
                </p>
                <h2>Motor Control and Face Pursuit Mode</h2>
                
                
                <h3>Motor Control</h3>
                
                
                <p>
                Sparky’s motor control largely comes from previously developed code for Lab 3 of ECE 5725. The code exists in two_wheel_mod.py and is responsible for initializing PWM signals for controlling the duty cycle of our motors. While the set duty cycle controls the turn speed, we can change the voltage sent to our control pins to also change the turn direction of our motors. With these functions combined, the file primarily includes a function <code>drive() </code>which makes it easier for making movement decisions with our robot. For example, it allows us to adjust for movement forward as well as individual motor spin speed. This is especially key to making micro adjustments to our travel path with Sparky when he is traveling forward. 
                </p>
                <h3>Face Pursuit</h3>
                
                
                <p>
                When locating a person from one of the three functions, <code>find_faces()</code>, <code>detect_faces_quick()</code>, or <code>identify_faces()</code> our robot Sparky will stop his motors and then determine an appropriate path for traveling to our desired target. Taking the center of our target as identified in our video feed, we can determine a relative offset from the center and therefore slow down the respective motors such that our robot will slowly turn towards our target while trying to maintain the target position at the center of Sparky’s vision. These conditions occur inside of <code>pursue_target()</code> and adjust our robot accordingly. We have this driving mode occur for a short period of time with our robot stuck inside a while loop, as we cannot process voice data during search mode anyways. When exiting, Sparky will then try to evaluate his position relative to the target. During this reevaluation, Sparky will then decide to do one of the three previously mentioned functions.
                </p>
                <p>
                As detailed earlier in the Face Recognition Implementation section, Sparky will use <code>detect_faces_quick()</code>, to just follow the face in front of him. If there is more than one or less than one face visible, we will default to doing a fully intensive <code>identify_faces()</code> call which provides us details on which of the correct faces to pursue. Because identify_faces() is so intensive for our image processing, we use a randomly generated number to allow Sparky to check using identify_faces about 1% of the time inside of detect_faces_quick in the scenario where this one visible subject. While not an ideal way of truly making our search process, use identify_faces only about 1% of the time, we found the solution to be good enough. In the case that we ever got a wrong reading for face recognition, which is unlikely, or our robot has lost sight of the target, Sparky will count the amount of consecutive misses and eventually return to the  <code>find_faces()</code> function if we cannot find our target. Lastly, the <code>find_faces()</code> operates by primarily having Sparky pivot counterclockwise, while slowly polling for the target face using <code>identify_faces()</code>. To start though, upon a new search request, Sparky will do a quick pivot left and then right as a fun gesture, making him act like a robo pet. In the case that the desired person is found, Sparky will then exit and make his travel towards the target. Sparky is more likely to find a person while turning slowly though. In addition, there is additional tuning movement which rotates Sparky back right by a bit as there tends to be some delay between the face recognition and when Sparky’s camera has already passed the target. Finally, after traveling to the requested person, Sparky’s search will come to a close after the desired target has reached an experimentally determined threshold value for proximity based on the amount of screen space the target person's head takes up in the captured video.
                </p>
              
              
              
              
              </p>
      </div>

  

      <div style="text-align:center;">
              <h2>Testing</h2>
              <p style="text-align: left;padding: 0px 30px;">Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum lorem nulla, consectetur at leo vel, pretium bibendum nisl. Cras blandit quam a enim ultrices, eu convallis enim posuere. Donec eleifend enim sed purus consectetur, vitae cursus lectus varius. Vivamus consectetur felis nec est venenatis posuere. Phasellus vitae aliquet erat. In laoreet lacinia mollis. Quisque iaculis nisl fermentum pharetra lobortis. Donec rhoncus dui sem, ac molestie leo tristique vel. Phasellus in nibh feugiat, fringilla lectus in, elementum magna. Etiam quis dui condimentum, tempus ex in, dapibus est. Cras ut congue augue. Donec ac enim ex. Ut id tristique risus, vel porttitor quam. Sed ultricies enim eu nibh porttitor, vel sodales enim feugiat. Fusce volutpat venenatis magna ac ultrices. Curabitur eget urna ut nulla mattis convallis non eu diam.</p>
      </div>

    <hr id='result'>

      <div style="text-align:center;">
              <h2>Result</h2>
              <p style="text-align: left;padding: 0px 30px;">Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum lorem nulla, consectetur at leo vel, pretium bibendum nisl. Cras blandit quam a enim ultrices, eu convallis enim posuere. Donec eleifend enim sed purus consectetur, vitae cursus lectus varius. Vivamus consectetur felis nec est venenatis posuere. Phasellus vitae aliquet erat. In laoreet lacinia mollis. Quisque iaculis nisl fermentum pharetra lobortis. Donec rhoncus dui sem, ac molestie leo tristique vel. Phasellus in nibh feugiat, fringilla lectus in, elementum magna. Etiam quis dui condimentum, tempus ex in, dapibus est. Cras ut congue augue. Donec ac enim ex. Ut id tristique risus, vel porttitor quam. Sed ultricies enim eu nibh porttitor, vel sodales enim feugiat. Fusce volutpat venenatis magna ac ultrices. Curabitur eget urna ut nulla mattis convallis non eu diam.</p>
      </div>

    <hr>

    <div class="row" style="text-align:center;">
          <h2>Work Distribution</h2>
          <div style="text-align:center;">
              <img class="img-rounded" src="pics/group.jpg" alt="Generic placeholder image" style="width:80%;">
              <h4>Project group picture</h4>
          </div>
          <div class="col-md-6" style="font-size:16px">
              <img class="img-rounded" src="pics/a.png" alt="Generic placeholder image" width="240" height="240">
              <h3>Rick</h3>
              <p class="lead">netid@cornell.edu</p>
              <p>Designed the overall software architecture (Just being himself).
          </div>
          <div class="col-md-6" style="font-size:16px">
              <img class="img-rounded" src="pics/b.png" alt="Generic placeholder image" width="240" height="240">
              <h3>Morty</h3>
              <p class="lead">netid@cornell.edu</p>
              <p>Tested the overall system.
          </div>
      </div>

    <hr>
      <div style="font-size:18px">
          <h2>Parts List</h2>
          <ul>
              <li>Raspberry Pi $35.00</li>
              <li>Raspberry Pi Camera V2 $25.00</li>
              <a href="https://www.adafruit.com/product/1463"><li>NeoPixel Ring - $9.95</li></a>
              <li>LEDs, Resistors and Wires - Provided in lab</li>
          </ul>
          <h3>Total: $69.95</h3>
      </div>
      <hr>
      <div style="font-size:18px">
          <h2>References</h2>
          <a href="https://picamera.readthedocs.io/">PiCamera Document</a><br>
          <a href="http://www.micropik.com/PDF/SG90Servo.pdf">Tower Pro Servo Datasheet</a><br>
          <a href="http://getbootstrap.com/">Bootstrap</a><br>
          <a href="http://abyz.co.uk/rpi/pigpio/">Pigpio Library</a><br>
          <a href="https://sourceforge.net/p/raspberry-gpio-python/wiki/Home/">R-Pi GPIO Document</a><br>

      </div>

    <hr>

      <div class="row">
              <h2>Code Appendix</h2>
              <pre><code>
// Hello World.c
int main(){
  printf("Hello World.\n");
}
              </code></pre>
      </div>

    </div><!-- /.container -->




    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
    <script src="dist/js/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <!-- <script src="../../assets/js/ie10-viewport-bug-workaround.js"></script> -->
  </body>
</html>

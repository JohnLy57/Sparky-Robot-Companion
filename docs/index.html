
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Starter Template for Bootstrap</title>

    <!-- Bootstrap core CSS -->
    <link href="dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

    <!-- Custom styles for this template -->
    <link href="starter-template.css" rel="stylesheet">

    <!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
    <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->
    <!-- <script src="../../assets/js/ie-emulation-modes-warning.js"></script> -->

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>

  <body>

    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="#">Sparky - Robot Companion</a>
        </div>
        <div id="navbar" class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li class="active"><a href="#">Home</a></li>
            <li><a href="#obj">Project Objective</a></li>
            <li><a href="#intro">Introduction</a></li>
            <li><a href="#design-and-testing">Design and Testing</a></li>
            <li><a href="#results">Results</a></li>
            <li><a href="#conclusion">Conclusion</a></li>
            <li><a href="#future-work">Future Work</a></li>
            <li><a href="#team">Team</a></li>
            <li><a href="#appendix">Appendix</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

    <div class="container">

      <div class="starter-template">
        <h1>Sparky - Robot Companion</h1>
        </p><p class="c51 subtitle" id="h.hpmrc74rl0fj">
	<p class="c20 c49 c5">Monday Section</p>
        </p><p class="c51 subtitle" id="h.hpmrc74rl0fj-1">
	<p class="c21"><p class="c5">ECE 5725 Final Project</h1>
	</p><p class="c51 subtitle" id="h.hpmrc74rl0fj-2">
	<p class="c20 c49 c5">By: John Ly (jtl22) and Carlos Gutierrez (cag334)</h1>
      </div>

      <hr>
      <div class="center-block">
          <iframe width="640" height="360" src="https://www.youtube.com/embed/dQw4w9WgXcQ" frameborder="0" allowfullscreen></iframe>
          <h4 style="text-align:center;">Demonstration Video</h4>
      </div>

      <hr id="obj ">

      <div style="text-align:center;">
              <h1>Project Objective:</h1>
	      </h1><p class="c3"><span class="c2">
	The goal for our ECE 5725: Design with Embedded Operating Systems final
        project was to create a smart mobile robot which uniquely combined OpenCV facial recognition alongside
	voice commands to create a fun robot capable of identifying faces and traveling to them upon voice requests.</span>
      </div>

    <hr id='intro'>

      <div style="text-align:center;">
          <h1 class="c46" id="h.vc2avdc0nzd7">
		<span class="c31 c20">Introduction</span>
	  </h1>
	<p class="c3"><span>Our final project is named Sparky. Sparky is an intelligent pet-like robot which listens to its 
		owner and is able to accomplish unique tricks in some ways similar to a real live trained dog. The basis for 
		this project is the Raspberry Pi 4 system combined with a piTFT display, a Raspberry Pi Camera, a microphone, 
		and a motorized chassis. Using both </span><span class="c19">
		<a class="c17" href="https://www.google.com/url?q=https://docs.opencv.org/master/index.html&amp;sa=D&amp;ust=1608452692603000&amp;usg=AOvVaw1b__CZFwsKjHQBZn78LL2g">OpenCV</a></span><span>&nbsp;and the Python </span><span class="c19"><a class="c17" href="https://www.google.com/url?q=https://github.com/ageitgey/face_recognition&amp;sa=D&amp;ust=1608452692603000&amp;usg=AOvVaw262bU8aGsOaLb-OJ-TZNxa">
		face_recognition</a></span><span>&nbsp;library, we are able to display on-screen video feedback of our camera 
		signal with additional image processing to detect faces. A connected microphone allows Sparky to actively record 
		sounds and listen for key words to act upon before doing a desired action, to do so Sparky uses the </span>
		<span class="c19"><a class="c17" href="https://www.google.com/url?q=https://github.com/Picovoice/picovoice&amp;sa=D&amp;ust=1608452692604000&amp;usg=AOvVaw3-pqobr9FeJur0Vvsh4bLt">
			picovoice</a></span><span>&nbsp;</span><span>library along with custom created files using </span>
		<span class="c19"><a class="c17" href="https://www.google.com/url?q=https://picovoice.ai/docs/quick-start/console-rhino/&amp;sa=D&amp;ust=1608452692604000&amp;usg=AOvVaw3moY7PoDSCXuNnMavxd84k">
			picovoice</a></span><span class="c19"><a class="c17" href="https://www.google.com/url?q=https://picovoice.ai/docs/quick-start/console-rhino/&amp;sa=D&amp;ust=1608452692604000&amp;usg=AOvVaw3moY7PoDSCXuNnMavxd84k">&nbsp;console</a>
		</span><span class="c2">. Lastly, the Raspberry Pi 4 is seated up above an acrylic frame in addition to a rechargeable 
		battery pack, 4 AA batteries for our two DC motors, the motor controller, and our two wheels for locomotion.</span>
	      </p>
	      
	      <p class="c3 c7"><span class="c2"></span></p>
        	<p class="c21">
            <span style="overflow: hidden; display: inline-block; margin: 0px 0px; border: 0px solid #000000; transform: rotate(0rad) translateZ(0px); -webkit-transform: rotate(0rad) translateZ(0px); width: 245px; height: 265.16px;">
                <img class="img-rounded" alt="" src="pics/image4.png" style="width: 304.7px; height: 303.48px; margin-left: -25.26px; margin-top: -38.31px; transform: rotate(0rad) translateZ(0px); -webkit-transform: rotate(0rad) translateZ(0px);" title="" />
            </span>
            <span style="overflow: hidden; display: inline-block; margin: 0px 0px; border: 0px solid #000000; transform: rotate(0rad) translateZ(0px); -webkit-transform: rotate(0rad) translateZ(0px); width: 210.26px; height: 268.5px;">
                <img class="img-rounded" alt="" src="pics/image7.png" style="width: 210.26px; height: 268.5px; margin-left: 0px; margin-top: 0px; transform: rotate(0rad) translateZ(0px); -webkit-transform: rotate(0rad) translateZ(0px);" title="" />
            </span>
        </p>
        <p class="c21"><span class="c2">Figure 1: Sparky Build - John&rsquo;s on left (mic not shown), Carlos&rsquo;s on right </span></p>
	<p class="c28"><span class="c7"></span></p>
	      
	      
	      <h3 class="c9" id="h.yy7kq6xc0jca"><span class="c20"><b>General Features</b></span></h3>
        <h4 class="c9" id="h.yy7kq6xc0jca"><span class="c20">The key programmed features are as follows:</span></h4>
	      <p class="c6"><span class="c7"><i>Voice Commands:</i></span></p>
        <p class="c6 c39">
            <span class="c7">
                At any moment while Sparky is sitting still and not performing any of the tasks the user can call Sparky by using his name and then after providing the correct structure sentence with the intentions that the user wants
                Sparky to perform. Once Sparky detects his name an LED will light up (similar to Alexa) signaling that whatever is said next will be processed as the intentions (voice commands) of the user and determine if what is said is
                understandable. The LED light will be on for 5 seconds and then turn off, during the time the LED light is on the user can speak to Sparky, if the light is to turn off the hotword Sparky will need to be said again in order
                to speak to Sparky.
            </span>
        </p> 
	</div>
	              <p class="c1 c39"><span class="c7"></span></p>
        <ul class="c12 lst-kix_ehvceqw0sdir-0 start">
            <li class="c6 c29"><span class="c7">Sparky (hot word for starting activation)</span></li>
            <li class="c6 c29"><span class="c7">Basic Movement</span></li>
        </ul>
        <ul class="c12 lst-kix_ehvceqw0sdir-1 start">
            <li class="c6 c54"><span class="c7">Go/Move/do a - (1)Forward, (2)Back, (3)Left, (4)Right, (5) Spin</span></li>
        </ul>
        <ul class="c12 lst-kix_ehvceqw0sdir-0">
            <li class="c6 c29"><span class="c7">Search</span></li>
        </ul>
        <ul class="c12 lst-kix_ehvceqw0sdir-1 start">
            <li class="c6 c54"><span class="c7">Find / Look for - (1)John, (2)Carlos</span></li>
        </ul>
        <ul class="c12 lst-kix_ehvceqw0sdir-0">
            <li class="c6 c29"><span class="c7">Tricks</span></li>
        </ul>
        <ul class="c12 lst-kix_ehvceqw0sdir-1 start">
            <li class="c6 c54"><span class="c7">Party Mode/Time</span></li>
            <li class="c6 c54"><span class="c7">Break Dance - (1)Hit It, (2)Now</span></li>
        </ul>
        <p class="c1"><span class="c7"></span></p>
	<div style="text-align:center;">
        <p class="c6"><span class="c7"><i>Face Identification:</i></span></p>
	</div>

        <ul class="c12 lst-kix_i2fegwf9o11y-0 start">
            <li class="c6 c29">
                <span class="c7">
                    Party Mode: most easily visible from party mode, Sparky will activate a 10s photo timer and additionally animate a party hat on any visible faces in front of Sparky. For recognized persons, they are given a proper party
                    hat, while unknown faces are given a simple yellow triangle hat. The photo is then snapshotted and printed at the bottom left corner of our piTFT. The files are additionally saved to be opened later if desired.
                </span>
            </li>
        </ul>
	<div style="text-align:center;">
        <p class="c40 c39">
            <span>&nbsp;</span>
            <span style="overflow: hidden; display: inline-block; margin: 0px 0px; border: 0px solid #000000; transform: rotate(0rad) translateZ(0px); -webkit-transform: rotate(0rad) translateZ(0px); width: 231.68px; height: 174.5px;">
                <img class="img-rounded" alt="Generic placeholder image" src="images/image8.jpg" style="width: 231.68px; height: 174.5px; margin-left: 0px; margin-top: 0px; transform: rotate(0rad) translateZ(0px); -webkit-transform: rotate(0rad) translateZ(0px);" title="" />
            </span>
            <span style="overflow: hidden; display: inline-block; margin: 0px 0px; border: 0px solid #000000; transform: rotate(0rad) translateZ(0px); -webkit-transform: rotate(0rad) translateZ(0px); width: 232.67px; height: 174.5px;">
                <img class="img-rounded" alt="Generic placeholder image" src="images/image3.png" style="width: 232.67px; height: 174.5px; margin-left: 0px; margin-top: 0px; transform: rotate(0rad) translateZ(0px); -webkit-transform: rotate(0rad) translateZ(0px);" title="" />
            </span>
        </p>
	</div>
        <ul class="c12 lst-kix_i2fegwf9o11y-0">
            <li class="c6 c29">
                <span class="c7">
                    Search Mode: upon activation, Sparky will do some quick movements in search of the requested person, either Carlos or John. After doing some in place rotations, upon recognition, Sparky will then stop and then
                    progressively drive towards the target until it is decided that the person requested is close enough to Sparky.
                </span>
            </li>
        </ul>
<div style="text-align:center;">
	<p class="c6"><span class="c7"><i>Untethered Operation:</i></span></p>
</div>
        <ul class="c12 lst-kix_dhd8drsquk4l-0 start">
            <li class="c6 c29">
                <span class="c7">
                    Sparky works independently of an internet connection and a wall-powered energy source. Sparky carries an on-board lithium rechargeable battery to power the Raspberry Pi and 4 AA batteries to deal with motor power.
                </span>
            </li>
        </ul>
<div style="text-align:center;">
	<p class="c6"><span class="c7"><i>Is a Robot Pet:</i></span></p>
       </div>
	<ul class="c12 lst-kix_70oyw0p5z5gv-0 start">
            <li class="c6 c29"><span>Cuteness is always a bonus point.</span></li>
        </ul>


<hr id='design-and-testing'>
      <div style="text-align:center;">
              <h3>Design and Testing</h3>
              <h2 class="c9" id="h.m6g2xz9z5jpl"><span class="c4">Build and Construction</span></h2>
        <p class="c6">
            <span style="overflow: hidden; display: inline-block; margin: 0px 0px; border: 0px solid #000000; transform: rotate(0rad) translateZ(0px); -webkit-transform: rotate(0rad) translateZ(0px); width: 624px; height: 314.67px;">
                <img class="img-rounded" alt="" src="images/image1.png" style="width: 624px; height: 314.67px; margin-left: 0px; margin-top: 0px; transform: rotate(0rad) translateZ(0px); -webkit-transform: rotate(0rad) translateZ(0px);" title="" />
            </span>
            <sup><a href="#cmnt3" id="cmnt_ref3">[c]</a></sup>
        </p>
        <p class="c40"><span class="c7">Figure 2: Sparky Wiring and Circuit Design</span></p>
        <p class="c28"><span class="c7"></span></p>
        <p class="c6">
            <span class="c7">
                The assembly of our robo pet builds off of a previous lab, Lab 3, reusing the same chassis, motors, motor controller, and battery design. The major additions are the camera, the microphone, and additional construction
                materials for mounting.
            </span>
        </p>
        <p class="c1"><span class="c7"></span></p>
        <p class="c6">
            <span class="c7">
                This project uses a Raspberry Pi 4 2GB with PiTFT Display on top. Connected to the display is a breakout cable which is inserted into a full sized breadboard. Here on this breadboard, we have wiring for our motor controller
                as well as our two DC motors, and Sparky&rsquo;s notification LED. These objects are all attached to a thin acrylic plate with mounting holes. While our overall construction is not ideal in terms of sturdiness, it is
                perfectly suitable for prototyping our design. Underneath and towards the back of our chassis is a ~2200mAh lithium ion battery responsible for directly powering our Raspberry Pi when Sparky is set to work untethered.
                Towards the front of the chassis is a 6V AA battery holder used to power our motors. Our motor controller is the Sparkfun TB6612FNG dual-channel motor controller as shown below. It gives us control of both independent motor
                operation as well as duty cycle to assist with making turns and changing speeds.
            </span>
        </p>
        <p class="c1"><span class="c7"></span></p>
        <p class="c40">
            <span class="c52">
                PWMA &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &rarr; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GPIO
                26&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PWMB &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &rarr; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GPIO
                16
            </span>
        </p>
        <p class="c40">
            <span class="c52">
                INA1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &rarr; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GPIO
                4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;INB1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &rarr;
                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GPIO 20
            </span>
        </p>
        <p class="c40">
            <span class="c52">
                INA2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp;&rarr; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GPIO
                5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;INB2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &rarr;
                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GPIO 21
            </span>
        </p>
        <p class="c40">
            <span style="overflow: hidden; display: inline-block; margin: 0px 0px; border: 0px solid #000000; transform: rotate(0rad) translateZ(0px); -webkit-transform: rotate(0rad) translateZ(0px); width: 445px; height: 206.17px;">
                <img class="img-rounded" alt="" src="images/image5.png" style="width: 445px; height: 206.17px; margin-left: 0px; margin-top: 0px; transform: rotate(0rad) translateZ(0px); -webkit-transform: rotate(0rad) translateZ(0px);" title="" />
            </span>
        </p>
	      
	        <p class="c40"><span class="c7">Sparkfun TB6612FNG dual-channel motor controller</span></p>
        <p class="c28"><span class="c7"></span></p>
        <p class="c6"><span class="c7">Shown below is the underside with our battery placement, motors, and wheels all easily visible. </span></p>
        <p class="c40">
            <span style="overflow: hidden; display: inline-block; margin: 0px 0px; border: 0px solid #000000; transform: rotate(0rad) translateZ(0px); -webkit-transform: rotate(0rad) translateZ(0px); width: 241.5px; height: 265.96px;">
                <img alt="" src="images/image6.png" style="width: 299.45px; height: 299.7px; margin-left: -29.25px; margin-top: -18.86px; transform: rotate(0rad) translateZ(0px); -webkit-transform: rotate(0rad) translateZ(0px);" title="" />
            </span>
        </p>
        <p class="c40"><span class="c7">Figure 4: Underside of Sparky</span></p>
        <p class="c28"><span class="c7"></span></p>
        <p class="c6">
            <span>As seen in </span><span class="c59">Figure 1</span><span>, the </span>
            <span class="c7">
                Raspberry Pi V2 Camera Module is placed directly facing forward and as high as possible upon the robot to get an angle closer to head level for face detection and recognition. Due to the constraints of our build, Sparky
                performs most optimally on a table surface where one&rsquo;s face is most visible from the camera&rsquo;s perspective due to the height. In a more advanced build there would be more room for an actuated camera with an
                additional degree of freedom to pivot and look around during operation. Our cameras are stood up onto our robot primarily using cardboard/wood splints. What is not displayed here is that Sparky additionally uses a USB
                connected mic for receiving audio. We individually found our own mics to use, but any small and lightweight omnidirectional microphone is ideal here. John substituted in a Blue Snowball iCE Microphone with a long wire while
                Carlos used a small headset and strapped it around Sparky.
            </span>
        </p>
        <p class="c1"><span class="c7"></span></p>
        <h2 class="c9" id="h.fqpc2o2v5ws1"><span class="c4">Voice recognition</span></h2>
        <h3 class="c9" id="h.ljge2ogfchbg"><span class="c20">Microphone Input:</span></h3>
        <p class="c6">
            <span>With any USB microphone connected to the Py we use the pyaudio() library to establish an audio stream from the microphone by setting variable </span><span class="c21">pa = pyaudio.PyAudio(). </span>
            <span>Next we open the audio stream</span><span class="c21">&nbsp;pa.open()</span><span>&nbsp;and set the sample rate, channels, format, and size of the buffer inside this variable (configuration based on the </span>
            <span>pyadudio</span><span>&nbsp;library). The values used for the </span><span class="c21">rate</span><span>,</span><span class="c21">&nbsp;format</span><span>&nbsp;and </span><span class="c21">frames_per_buffer</span>
            <span class="c7">
                &nbsp;are taken from the picovoice library because we will later process the audio stream using the picovoice library and there must be congruence between the audio format and the necessary requirements of the picovoice
                library. The audio_stream variable setup is shown below.
            </span>
        </p>
        <p class="c1"><span class="c7"></span></p>
        <a id="t.64850c0bda086c0d23798a0ccb6423a60abe8f45"></a><a id="t.0"></a>
<div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">audio_stream <span style="color: #333333">=</span> pa<span style="color: #333333">.</span>open(
	rate<span style="color: #333333">=</span>_picovoice<span style="color: #333333">.</span>sample_rate,
	channels<span style="color: #333333">=</span><span style="color: #0000DD; font-weight: bold">1</span>,
	format<span style="color: #333333">=</span>pyaudio<span style="color: #333333">.</span>paInt16,
	<span style="color: #007020">input</span><span style="color: #333333">=</span><span style="color: #007020">True</span>,
	frames_per_buffer<span style="color: #333333">=</span>_picovoice<span style="color: #333333">.</span>frame_length)
</pre></div>

        <p class="c1"><span class="c14 c35"></span></p>
        <h3 class="c9" id="h.5s2mcfodle19"><span class="c59">Using Picovoice():</span><span class="c20">&nbsp;</span></h3>
        <p class="c6">
            <span>
                The picovoice engine accepts 16-bit linearly-encoded PCM and operates on a single-channel. The audio is processed in consecutive frames. Picovoice is able to understand wake words (hotword) and after the recognition of the
                wake word it detects the intentions of the user using inference. Both the wake word and the inference are customizable and can be created using the online picovoice console, after the desired phrases are created and trained
                online we can download the trained files that pertain to the wake word and inferences. The wake word is .ppn type file and the inference is a .rhn type file. The path to these files are placed inside variable
            </span>
            <span class="c21">_picovoice</span><span class="c7">&nbsp;along with other configurations seen below.</span>
        </p>
        <p class="c1"><span class="c7"></span></p>
        <a id="t.e15e6e9e10a205a62fe9162bc6e9dbdeed73a4bd"></a><a id="t.1"></a>
        <!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">_picovoice<span style="color: #333333">=</span>Picovoice(
	keyword_path<span style="color: #333333">=</span>_keyword_path,
	wake_word_callback<span style="color: #333333">=</span>wake_word_callback,
	context_path<span style="color: #333333">=</span>_context_path,
	inference_callback<span style="color: #333333">=</span>inference_callback,
	porcupine_library_path<span style="color: #333333">=</span><span style="color: #007020">None</span>,
	porcupine_model_path<span style="color: #333333">=</span><span style="color: #007020">None</span>,
	rhino_sensitivity<span style="color: #333333">=</span><span style="color: #6600EE; font-weight: bold">0.2</span>, 
	rhino_library_path<span style="color: #333333">=</span><span style="color: #007020">None</span>,
	rhino_model_path<span style="color: #333333">=</span><span style="color: #007020">None</span>,
	porcupine_sensitivity<span style="color: #333333">=</span><span style="color: #0000DD; font-weight: bold">1</span>)
	</pre></div>

        <p class="c1"><span class="c7"></span></p>
        <p class="c6">
            <span>Variables</span><span class="c21">&nbsp;wake_word_callback</span><span>&nbsp;and</span><span class="c21">&nbsp;inferecence_callback</span>
            <span>&nbsp;are assigned to a respective function that handles the signal of such an event occurring. Variables </span><span class="c21">rhino_sensitivity</span><span>&nbsp;and</span>
            <span class="c21">&nbsp;porcupine_sensitivy</span>
            <span class="c7">
                &nbsp;are assigned a value from [0-1] where 1 is the most sensitive value for the library to react to inferences and to the wake word respectively. We set the hot word sensitivity to the 1 to make sure a call to Sparky will
                be very easily recognized. The value set for the inference is low to make sure that the spoken commands are correctly ottered and so that the library is not set off by incorrectly uttered commands &nbsp;that are not part of
                Sparky&rsquo;s understanding.
            </span>
        </p>
        <p class="c6">
            <span>We opted for our audio to be constantly running and processing the information gathered from the microphone, therefore at the beginning of the while loop we set variable</span><span class="c21">&nbsp;</span>
            <span class="c21 c15">pcm = audio_stream.read(_picovoice.frame_length,exception_on_overflow=</span><span class="c0">False</span><span class="c21 c15">)</span>
            <span class="c15">&nbsp;in order to read the microphone, on the next line we edit the variable </span><span class="c21 c15">pcm = struct.unpack_from(&quot;h&quot; * _picovoice.frame_length, pcm) </span>
            <span class="c15">to unpack the information from the audio into a format picovoice expects and finally we process the data</span><span class="c21 c15">&nbsp;_picovoice.process(pcm)</span><span class="c14 c15">.</span>
        </p>
        <p class="c1"><span class="c14 c15"></span></p>
        <p class="c6"><span class="c55">def wake_word_callback():</span></p>
        <p class="c6">
            <span class="c7">
                This function is accessed when the word &ldquo;Sparky&rdquo; is recognized by picovoice engine. Here we set all mode flags to false, light up the LED connected to GPIO pin 13, set flag intention.led_on to True which will
                allow us to turn off the LED after 5 seconds, we also stop the motors of the robot. The main purpose behind this is to give the sense that when Sparky is called he stops what he is doing and listens to the next instructions.
            </span>
        </p>
        <p class="c1"><span class="c7"></span></p>
        <p class="c6"><span class="c55">def inference_callback():</span></p>
        <p class="c6">
            <span>After the wake word is recognized and the inferences spoken are understood this function accessed in here we enable the flags that set off the modes inside the while based on the values of</span>
            <span class="c21">&nbsp;inference.intent</span><span>&nbsp;which can be either &lsquo;move&rsquo; or search&rsquo; strings. Variable </span><span class="c21">inference. slots[] </span>
            <span>is a dictionary type and contains the value of slot under the uttered intent. Along with enabling the correct flag we write the value of the action Sparky is taking into variable </span>
            <span class="c21">intention.word</span><span>&nbsp;as a string ( used later on to write on the TFT display). We initiate variable </span><span class="c21">timerStart= time.time()</span>
            <span class="c7">&nbsp;to be used in modes where a time limit is implemented, we read this value to know when the mode began. </span>
        </p>
        <p class="c1"><span class="c7"></span></p>
        <p class="c1"><span class="c7"></span></p>
        <h2 class="c9" id="h.fqs47bnu625b"><span class="c4">Face Detection and Recognition</span></h2>
        <h3 class="c9" id="h.7urs1bchb1jk"><span class="c20">Face Detection Algorithm</span></h3>
        <p class="c6">
            <span>
                Overall while many face detection and recognition libraries, choosing an appropriate method for our device had some challenges. Initially, we developed Sparky using the Local Binary Patterns Histograms (LBPH) algorithm
                available through the OpenCV library. More information about how this method works can be found
            </span>
            <span class="c18">
                <a class="c5" href="https://www.google.com/url?q=https://towardsdatascience.com/face-recognition-how-lbph-works-90ec258c3d6b&amp;sa=D&amp;ust=1608455692521000&amp;usg=AOvVaw1tlv5h1CktDHQVWBHmlZ-d">here</a>
            </span>
            <span class="c7">
                , but in summary the algorithm takes in grayscale images and uses thresholding to turn the pixel array into binary data. From here a histogram of the data is created and based on new incoming images. Those images are treated
                similarly and compared across various histogram values. Based on some differences between the values, a confidence value can be calculated to determine a person&rsquo;s face. Initially this method seemed to work fine,
                properly identifying and labeling John&rsquo;s face, but after further testing, we found that while the algorithm works okay for already pretrained images, it had some biases towards already trained faces and resulted in
                lots of errors especially with faces that were supposed to be unknown to the system.
            </span>
        </p>
        <p class="c1"><span class="c7"></span></p>
        <p class="c6">
            <span>As a result, towards the later half of the project we pivoted towards using the significantly more accurate face_recognition library linked </span>
            <span class="c18"><a class="c5" href="https://www.google.com/url?q=https://github.com/ageitgey/face_recognition&amp;sa=D&amp;ust=1608455692521000&amp;usg=AOvVaw32gNUfd3bipSotWZ9lFvnj">here</a></span>
            <span>. According to the documentation, this library was trained using the </span>
            <span class="c18"><a class="c5" href="https://www.google.com/url?q=http://dlib.net/&amp;sa=D&amp;ust=1608455692521000&amp;usg=AOvVaw1BEDZ_Vp8n1pwKJBxUZFa3">Dlib C++</a></span>
            <span>
                &nbsp;library along with deep learning methods. The result here is that key facial features are determined primarily through color gradient changes within a face. These results are recorded in a histogram of oriented
                gradients (HOG) which is then encoded for usage with our facial recognition model. With these gradients it is possible to discern key landmark features such as the eye shape, eyebrows, nose shape, mouth, and jawline.
                Additional details on the deep learning algorithm used can be found
            </span>
            <span class="c18">
                <a
                    class="c5"
                    href="https://www.google.com/url?q=https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78&amp;sa=D&amp;ust=1608455692522000&amp;usg=AOvVaw3uu1CJJl383FPQDHhZLw7y"
                >
                    here
                </a>
            </span>
            <span class="c7">.</span>
        </p>
        <p class="c1"><span class="c7"></span></p>
        <p class="c6">
            <span class="c7">
                In our pivot from using the LBPH model method provided in OpenCV to the face_recogniton library written by ageitgey, there were a few major resulting changes. Firstly, face recognition was significantly improved and with
                very high accuracy. The major tradeoff though was that the necessary computing power was much more significant and resulted in our smooth video feed losing many frames. A few factors are the cause for this, but some
                potential solutions for improving computation times would be having a processor better suited for calculated parallel computations and with better single core performance. On a standard desktop, this can be achieved through
                using Nvidia&rsquo;s CUDA cores or a high performance CPU. One potential other issue is that our compilation of dlib using pip3 install may have some strange performance errors as opposed to compiling the library on board
                our Raspberry Pi. This has not been tested though as this potential solution was only recently found by us.
            </span>
        </p>
        <p class="c1"><span class="c7"></span></p>
        <h3 class="c9" id="h.h0e6opqz7yzn"><span class="c20">Face Recognition Training Data</span></h3>
        <p class="c6">
            <span>
                As Sparky is currently, he only can recognize pretrained faces. It would very much be possible to have active face recognition work with new faces during operation but we&rsquo;ll leave that for a possible future
                implementation. Using this
            </span>
            <span class="c18"><a class="c5" href="https://www.google.com/url?q=https://www.tomshardware.com/how-to/raspberry-pi-facial-recognition&amp;sa=D&amp;ust=1608455692523000&amp;usg=AOvVaw0kYYIJMx2n51GnimXnOmj2">guide</a></span>
            <span>&nbsp;</span>
            <span class="c7">
                from Caroline Dunn, we were able to make a face recognition system that ran using active video data instead of just identifying still images as provided by the face_recognition library. To first train our encodings for face
                recognition, we start by taking a collection of photos of primarily one&#39;s face while rotated to different angles. For John, he additionally took photos with and without glasses. We used about 20-30 images but the
                face_detection library is supposedly very capable with even just 10 images. With our input data of identifying names and images, we first take our image data captured in BGR format and convert it to RGB before using the face
                recognition library to determine face positions as well as create encodings in HOG model format.
            </span>
        </p>

        <div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">boxes <span style="color: #333333">=</span> face_recognition<span style="color: #333333">.</span>face_locations(rgb, model<span style="color: #333333">=</span><span style="background-color: #fff0f0">&quot;hog&quot;</span>)
    encodings <span style="color: #333333">=</span> face_recognition<span style="color: #333333">.</span>face_encodings(rgb, boxes)
    <span style="color: #008800; font-weight: bold">for</span> encoding <span style="color: #000000; font-weight: bold">in</span> encodings:
        knownEncodings<span style="color: #333333">.</span>append(encoding)
        knownNames<span style="color: #333333">.</span>append(name)
	</pre></div>
the Python pickle library allows us to save our data into a serialized format.</span></p>
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">data <span style="color: #333333">=</span> {<span style="background-color: #fff0f0">&quot;encodings&quot;</span>: knownEncodings, <span style="background-color: #fff0f0">&quot;names&quot;</span>: knownNames}
f <span style="color: #333333">=</span> <span style="color: #007020">open</span>(<span style="background-color: #fff0f0">&quot;encodings.pickle&quot;</span>, <span style="background-color: #fff0f0">&quot;wb&quot;</span>)
f<span style="color: #333333">.</span>write(pickle<span style="color: #333333">.</span>dumps(data))
f<span style="color: #333333">.</span>close()
</pre></div>

        <h3 class="c9" id="h.xmn0kfz2g6vq"><span class="c20">Face Recognition Implementation</span></h3>
        <p class="c6">
            <span>For both face recognition algorithms described, our system first required face detection using OpenCV&rsquo;s </span>
            <span class="c10">
                Haar Feature-based Cascade Classifier model which is able to quickly detect objects, but in our case faces. The difference between face detection and face recognition is that recognition is more specific and allows for us to
                distinguish between faces whereas face detection only provides us with the ability to determine if a human face is present. The model is available pre-trained but is generally created using lots of positive (faces) and
                negative (no faces) images. From there, the differences allow for key features to be extracted and to create a model. The return from this face detection when using
            </span>
            <span class="c37 c60">detector</span><span class="c37 c63">.detectMultiScale() </span><span class="c10">where detector is a </span><span class="c37 c63">CascadeClassifier</span>
            <span class="c14 c10">
                &nbsp;is a set of 4 values describing the pixel values of a box surrounding the detected person&rsquo;s face from within an image. The format of this is (x,y,w,h) where x and y describe the starting position of the box from
                the top left corner while w and h describe the pixel width and height of the box respectively. While this classifier only uses grayscale images, our face recognition algorithm uses RGB format images thus we must create both
                forms of images. For those unfamiliar, it is also important to note that in pixel arrays (0,0) starts from the top left corner.
            </span>
        </p>
    <div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">  img_gray <span style="color: #333333">=</span> cv2<span style="color: #333333">.</span>cvtColor(img, cv2<span style="color: #333333">.</span>COLOR_BGR2GRAY)
    img_rgb <span style="color: #333333">=</span> cv2<span style="color: #333333">.</span>cvtColor(img, cv2<span style="color: #333333">.</span>COLOR_BGR2RGB)
 
    <span style="color: #888888"># simple face detection</span>
    faces <span style="color: #333333">=</span> detector<span style="color: #333333">.</span>detectMultiScale( img_gray, scaleFactor <span style="color: #333333">=</span> <span style="color: #6600EE; font-weight: bold">1.3</span>, 
minNeighbors <span style="color: #333333">=</span> <span style="color: #0000DD; font-weight: bold">5</span>, minSize <span style="color: #333333">=</span> (<span style="color: #007020">int</span>(minW), <span style="color: #007020">int</span>(minH)), 
flags <span style="color: #333333">=</span> cv2<span style="color: #333333">.</span>CASCADE_SCALE_IMAGE
    )
</pre></div>

            <span class="c10">Using this data we are able to zoom in on the data that we want to process for face recognition and individually compare each boxed face to our encoded HOG dataset for identification. While the function </span>
            <span class="c37 c60">detect_faces_quick() </span><span class="c10">primarily just uses the above lines of code for face detection, the function </span><span class="c37 c60">identify_faces()</span>
            <span class="c37 c60 c66">&nbsp;</span>
            <span class="c14 c10">
                is used for proper face recognition and deals with the comparisons as previously mentioned. Using new camera data and previously trained encodings created from the face_recogntion library, we can make our identifications and
                label the faces accordingly.
            </span>
        </p>
        <p class="c1"><span class="c14 c10"></span></p>
        <p class="c6">
            <span class="c14 c10">
                In addition to the OpenCV library being responsible for opening up our camera and reading the incoming signal, we also used it for drawing and labeling on top of our images. The library includes the ability to draw shapes
                and write text as desired at different pixel coordinates. Some of that can be seen below. In the case that we cannot identify a person, the written name is defaulted to &ldquo;Unknown&rdquo; and they are labeled as such.
            </span>
        </p>
        <p class="c1"><span class="c14 c10"></span></p>
        <p class="c22">
         <div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"> cv2<span style="color: #333333">.</span>ellipse(img, (center[<span style="color: #0000DD; font-weight: bold">0</span>], center[<span style="color: #0000DD; font-weight: bold">1</span>]), (w<span style="color: #333333">//</span><span style="color: #0000DD; font-weight: bold">2</span>, h<span style="color: #333333">//</span><span style="color: #0000DD; font-weight: bold">2</span>), <span style="color: #0000DD; font-weight: bold">0</span>, <span style="color: #0000DD; font-weight: bold">0</span>, <span style="color: #0000DD; font-weight: bold">360</span>, (<span style="color: #0000DD; font-weight: bold">255</span>, <span style="color: #0000DD; font-weight: bold">0</span>, <span style="color: #0000DD; font-weight: bold">255</span>), <span style="color: #0000DD; font-weight: bold">2</span>)   
  cv2<span style="color: #333333">.</span>putText(img, <span style="color: #007020">str</span>(name), (leftX<span style="color: #333333">-</span><span style="color: #0000DD; font-weight: bold">15</span>,bottomY<span style="color: #333333">+</span><span style="color: #0000DD; font-weight: bold">30</span>), font, <span style="color: #6600EE; font-weight: bold">1.2</span>, (<span style="color: #0000DD; font-weight: bold">255</span>,<span style="color: #0000DD; font-weight: bold">255</span>,<span style="color: #0000DD; font-weight: bold">255</span>), <span style="color: #0000DD; font-weight: bold">2</span>)
</pre></div>

        </p>

        <p class="c1"><span class="c14 c10"></span></p>
        <h3 class="c9" id="h.n7fchvxyc524"><span class="c20">Move mode:</span></h3>
        <p class="c6">
            <span>When </span><span class="c21">instruction.v_direction</span>
            <span>&nbsp;is True, Sparky&rsquo;s motors move in the direction specified by the user that direction is read from variable instruction.word. For each of the directions there is a call to function </span>
            <span class="c21">movement_3sec(&ldquo;direction&rdquo;,speed, speed)</span><span>&nbsp;which will make the motors move in the &ldquo;direction&rdquo; uttered. Inside the function</span>
            <span class="c21">&nbsp;movement_3sec()</span>
            <span class="c7">
                &nbsp;a timer is implemented for directions (&#39;forward&#39;, &#39;backward&#39;, &#39;right&#39;, and &nbsp;&#39;left&#39;) of half a second implemented to make Sparky stop after the timer is over. Using pygame we also
                update the TFT display message to display what Sparky is doing.
            </span>
        </p>
        <p class="c1"><span class="c7"></span></p>
        <a id="t.04eb9b9f37e42194dcc2ae6ec7b71781a5aec546"></a><a id="t.2"></a>
        <!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #008800; font-weight: bold">if</span> instruction<span style="color: #333333">.</span>v_direction:
	<span style="color: #008800; font-weight: bold">if</span> instruction<span style="color: #333333">.</span>word <span style="color: #333333">==</span> <span style="background-color: #fff0f0">&#39;forward&#39;</span>:
				movement_3sec(<span style="background-color: #fff0f0">&#39;forward&#39;</span>,<span style="color: #0000DD; font-weight: bold">90</span>,<span style="color: #0000DD; font-weight: bold">90</span>)
	<span style="color: #008800; font-weight: bold">if</span> instruction<span style="color: #333333">.</span>word <span style="color: #333333">==</span> <span style="background-color: #fff0f0">&#39;back&#39;</span>:
				movement_3sec(<span style="background-color: #fff0f0">&#39;backward&#39;</span>,<span style="color: #0000DD; font-weight: bold">90</span>,<span style="color: #0000DD; font-weight: bold">90</span>)
	<span style="color: #008800; font-weight: bold">if</span> instruction<span style="color: #333333">.</span>word <span style="color: #333333">==</span> <span style="background-color: #fff0f0">&#39;right&#39;</span>:
				movement_3sec(<span style="background-color: #fff0f0">&#39;right&#39;</span>,<span style="color: #0000DD; font-weight: bold">90</span>,<span style="color: #0000DD; font-weight: bold">90</span>)
	<span style="color: #008800; font-weight: bold">if</span> instruction<span style="color: #333333">.</span>word <span style="color: #333333">==</span> <span style="background-color: #fff0f0">&#39;left&#39;</span>:
				movement_3sec(<span style="background-color: #fff0f0">&#39;left&#39;</span>,<span style="color: #0000DD; font-weight: bold">90</span>,<span style="color: #0000DD; font-weight: bold">90</span>)
	<span style="color: #008800; font-weight: bold">if</span> instruction<span style="color: #333333">.</span>word <span style="color: #333333">==</span> <span style="background-color: #fff0f0">&#39;spin&#39;</span>:
				tw<span style="color: #333333">.</span>drive(<span style="background-color: #fff0f0">&quot;left&quot;</span>,<span style="color: #0000DD; font-weight: bold">90</span>,<span style="color: #0000DD; font-weight: bold">90</span>)
			
	screen<span style="color: #333333">.</span>fill(BLACK, textAreaRect)
	text <span style="color: #333333">=</span> fontLg<span style="color: #333333">.</span>render(f<span style="background-color: #fff0f0">&#39;Moving {instruction.word}&#39;</span>, <span style="color: #007020">True</span>, WHITE, BLACK)
	textRect <span style="color: #333333">=</span> text<span style="color: #333333">.</span>get_rect(center<span style="color: #333333">=</span>(<span style="color: #6600EE; font-weight: bold">0.5</span><span style="color: #333333">*</span>dispW, <span style="color: #6600EE; font-weight: bold">0.75</span><span style="color: #333333">*</span>dispH))
	screen<span style="color: #333333">.</span>fill(BLACK, textRect<span style="color: #333333">.</span>inflate(<span style="color: #0000DD; font-weight: bold">240</span>,<span style="color: #0000DD; font-weight: bold">0</span>))
            screen<span style="color: #333333">.</span>blit(text,textRect)
            pygame<span style="color: #333333">.</span>display<span style="color: #333333">.</span>update(textRect)
</pre></div>

        <p class="c1"><span class="c7"></span></p>
        <p class="c1"><span class="c7"></span></p>
        <h3 class="c9" id="h.pfc7pl10t4f7"><span>Search Mode:</span></h3>
        <p class="c6">
            <span class="c35">When variable </span><span class="c21 c35">instruction.v_search</span>
            <span class="c35">
                &nbsp;is true we can tell Sparky to begin the process of finding the user specified. First the text on theTFT is updated to show the change in mode, Sparky displays &lsquo;Looking for &lsquo;user&rsquo;...&rsquo;. A timer
                condition of 20 seconds is set so that Sparky only performs the task of searching for 20 seconds. Inside this condition there is a call to the function
            </span>
            <span class="c21 c35">foundFace = find_faces(instruction.word, img)</span><span class="c35">. If the function returns True, that means the target user was found in that direction, then we stop running the</span>
            <span class="c21 c35">&nbsp;find_faces</span><span class="c35">&nbsp;function and we proceed to running</span><span class="c35 c59">&nbsp;</span><span class="c35">the </span><span class="c21 c35">detect_faces_quick</span>
            <span class="c35">&nbsp;function. Our return value,</span><span class="c21 c35">&nbsp;facePresent = detect_faces_quick(img)</span><span class="c35">will be t</span>
            <span class="c35">rue if there is a singular detectable face still in the view of the camera. If facePresent is true we perform </span><span class="c21 c35">pursue_target(True)</span>
            <span class="c35">&nbsp;which will guide the robot closer towards the face based on the location of the face on the image from the camera. If </span><span class="c21 c35">facePresent</span>
            <span class="c35">&nbsp;is false then we run the function </span><span class="c21 c35">identify_faces()</span>
            <span class="c35">&nbsp;which determines if the user requested is found in the field of view of the camera, this function then returns a value for </span><span class="c21 c35">target</span>
            <span class="c35">&nbsp;and then again we perform</span><span class="c21 c35">&nbsp;pursue_target</span><span class="c35">&nbsp;if</span><span class="c21 c35">&nbsp;target=True.</span><span class="c35">I</span>
            <span class="c35">n the case of</span><span class="c21 c35">&nbsp;target= False</span><span class="c35">&nbsp;we increment variable</span><span class="c21 c35">&nbsp;misses</span>
            <span class="c35">, after 10 consecutive misses (</span><span class="c21 c35">misses &gt;10</span><span class="c35">) we then try again and run the </span><span class="c21 c35">find_faces</span>
            <span class="c14 c35">&nbsp;function. </span>
        </p>
        <p class="c1"><span class="c14 c35"></span></p>
        <p class="c6">
            <span class="c35">
                The main reason for dividing the face recognition steps like this is to make an adjustment between running a very heavy face recognition process, which provides the best results on identifying the user, and being able to
                pursuing the target in the correct direction. If the heavy face recognition function is always running it has an enormous effect on the framerate of the camera which then translates to erroneous movements of the robot
                because we are moving faster than what our image processing can return. This causes a delay on the reaction of the motors and a bad performance by Sparky. We opted to implement the search algorithm this way so that after
                finding our target our driving motion towards the face consists of using face detection only for approximately 95% of the time (light on the processor) and then we use proper face recognition for the remaining other 5% of
                the time (heavy on the processor). This way we make sure that Sparky makes faster motor adjustment decisions when moving towards the target and also that he recognizes the face with high precision. Once Sparky reaches the
                target user, the flag
            </span>
            <span class="c21 c35">&nbsp;stopCondition</span><span class="c35">&nbsp;is set and we exit the search mode (set</span><span class="c21 c35">&nbsp;instruction.v_search =False</span>
            <span class="c14 c35">
                ) and set all variables back to their starting value, we also update the TFT display to notify that the user was found. While Sparky is trying to find the user, a counter is displayed onto the screen showing how much time
                has elapsed since the search began, and as previously stated once the count reaches its limit of 20 seconds, Sparky stops the search.
            </span>
        </p>
        <h3 class="c9" id="h.9hhfn7akun2k"><span class="c20">Tricks Mode:</span></h3>
        <p class="c6">
            <span>For Sparky to go into this mode the variabel</span><span class="c21">&nbsp;instruction.v_tricks</span><span>&nbsp;is set to true by the voice recognition. Inside this condition we look at variable</span>
            <span class="c21">&nbsp;instruction.word</span><span class="c7">&nbsp;and determine if it&#39;s equal to &lsquo;party&rsquo; or &lsquo;break dance&rsquo;, the two tricks that Sparky can do. </span>
        </p>
        <p class="c1"><span class="c7"></span></p>
        <p class="c6">
            <span>If </span><span class="c21">instruction.word == &lsquo;party&rsquo;, </span><span>we then again update the tft display to show what Sparky is doing and we run function </span>
            <span class="c21">_,_,img = identify_faces(None, img, mode = &quot;party_time&quot;)</span>
            <span class="c7">
                &nbsp;this is the same face detection function previously used by the search mode but since in this case the motors for Sparky are not running we take the privilege to run it without constraints. We pass mode=
                &ldquo;party_time&rdquo; to the function to tell it that it will not only do face recognition but will also draw on top of the user the trade-marked party hats. For users that Sparky knows (Carlos and John) the hat is
                special and for people that Sparky doesn&rsquo;t know yet they will receive just an outlined triangle. This mode will run for 10 seconds and also display a 10 second counter on the screen at the end of the time Sparky exits
                the mode and waits for his next instructions.
            </span>
        </p>
        <p class="c1"><span class="c7"></span></p>
        <p class="c6">
            <span style="overflow: hidden; display: inline-block; margin: 0px 0px; border: 0px solid #000000; transform: rotate(0rad) translateZ(0px); -webkit-transform: rotate(0rad) translateZ(0px); width: 219px; height: 293.5px;">
                <img alt="" src="images/image10.gif" style="width: 219px; height: 293.5px; margin-left: 0px; margin-top: 0px; transform: rotate(0rad) translateZ(0px); -webkit-transform: rotate(0rad) translateZ(0px);" title="" />
            </span>
            <sup><a href="#cmnt4" id="cmnt_ref4">[d]</a></sup><sup><a href="#cmnt5" id="cmnt_ref5">[e]</a></sup><sup><a href="#cmnt6" id="cmnt_ref6">[f]</a></sup><sup><a href="#cmnt7" id="cmnt_ref7">[g]</a></sup>
            <sup><a href="#cmnt8" id="cmnt_ref8">[h]</a></sup><sup><a href="#cmnt9" id="cmnt_ref9">[i]</a></sup><sup><a href="#cmnt10" id="cmnt_ref10">[j]</a></sup>
            <span style="overflow: hidden; display: inline-block; margin: 0px 0px; border: 0px solid #000000; transform: rotate(0rad) translateZ(0px); -webkit-transform: rotate(0rad) translateZ(0px); width: 390.5px; height: 292.61px;">
                <img alt="" src="images/image9.png" style="width: 390.5px; height: 292.61px; margin-left: 0px; margin-top: 0px; transform: rotate(0rad) translateZ(0px); -webkit-transform: rotate(0rad) translateZ(0px);" title="" />
            </span>
        </p>
        <p class="c1"><span class="c7"></span></p>
        <p class="c6">
            <span>If </span><span class="c21">instruction.word == &lsquo;break dance&rsquo;</span>
            <span class="c7">
                , then TFT display is updated and a function call to move_breakdance() is made. This function moves the motors so that sparky alternates between moving right and left. Sparky looks like he is wiggling in place.
            </span>
        </p>
        <p class="c6">
            <span style="overflow: hidden; display: inline-block; margin: 0px 0px; border: 0px solid #000000; transform: rotate(0rad) translateZ(0px); -webkit-transform: rotate(0rad) translateZ(0px); width: 293.5px; height: 348.99px;">
                <img alt="" src="images/image2.gif" style="width: 293.5px; height: 348.99px; margin-left: 0px; margin-top: 0px; transform: rotate(0rad) translateZ(0px); -webkit-transform: rotate(0rad) translateZ(0px);" title="" />
            </span>
        </p>
        <p class="c1"><span class="c7"></span></p>
        <h2 class="c9" id="h.20mlxkp95k4m"><span class="c4">Motor Control and Face Pursuit Mode</span></h2>
        <h3 class="c9" id="h.7jr0avd5e1nc"><span class="c20">Motor Control</span></h3>
        <p class="c6">
            <span>
                Sparky&rsquo;s motor control largely comes from previously developed code for Lab 3 of ECE 5725. The code exists in two_wheel_mod.py and is responsible for initializing PWM signals for controlling the duty cycle of our
                motors. While the set duty cycle controls the turn speed, we can change the voltage sent to our control pins to also change the turn direction of our motors. With these functions combined, the file primarily includes a
                function
            </span>
            <span class="c37">drive() </span>
            <span class="c7">
                which makes it easier for making movement decisions with our robot. For example, it allows us to adjust for movement forward as well as individual motor spin speed. This is especially key to making micro adjustments to our
                travel path with Sparky when he is traveling forward.
            </span>
        </p>
        <h3 class="c9" id="h.30fzgvmhfpvd"><span class="c20">Face Pursuit</span></h3>
        <p class="c6">
            <span>When locating a person from one of the three functions, </span><span class="c37">find_faces()</span><span>, </span><span class="c37">detect_faces_quick()</span><span>, or </span><span class="c37">identify_faces()</span>
            <span>
                &nbsp;our robot Sparky will stop his motors and then determine an appropriate path for traveling to our desired target. Taking the center of our target as identified in our video feed, we can determine a relative offset from
                the center and therefore slow down the respective motors such that our robot will slowly turn towards our target while trying to maintain the target position at the center of Sparky&rsquo;s vision. These conditions occur
                inside of
            </span>
            <span class="c37">pursue_target()</span>
            <span class="c7">
                &nbsp;and adjust our robot accordingly. We have this driving mode occur for a short period of time with our robot stuck inside a while loop, as we cannot process voice data during search mode anyways. When exiting, Sparky
                will then try to evaluate his position relative to the target. During this reevaluation, Sparky will then decide to do one of the three previously mentioned functions.
            </span>
        </p>
        <p class="c1"><span class="c7"></span></p>
        <p class="c6">
            <span>As detailed earlier in the Face Recognition Implementation section, Sparky will use </span><span class="c37">detect_faces_quick()</span>
            <span>, to just follow the face in front of him. If there is more than one or less than one face visible, we will default to doing a fully intensive </span><span class="c37">identify_faces()</span>
            <span>
                &nbsp;call which provides us details on which of the correct faces to pursue. Because identify_faces() is so intensive for our image processing, we use a randomly generated number to allow Sparky to check using
                identify_faces about 1% of the time inside of detect_faces_quick in the scenario where this one visible subject. While not an ideal way of truly making our search process, use identify_faces only about 1% of the time, we
                found the solution to be good enough. In the case that we ever got a wrong reading for face recognition, which is unlikely, or our robot has lost sight of the target, Sparky will count the amount of consecutive misses and
                eventually return to the &nbsp;
            </span>
            <span class="c37">find_faces()</span><span>&nbsp;function if we cannot find our target. Lastly, the </span><span class="c37">find_faces()</span>
            <span>&nbsp;operates by primarily having Sparky pivot counterclockwise, while slowly polling for the target face using </span><span class="c37">identify_faces()</span>
            <span class="c7">
                . To start though, upon a new search request, Sparky will do a quick pivot left and then right as a fun gesture, making him act like a robo pet. In the case that the desired person is found, Sparky will then exit and make
                his travel towards the target. Sparky is more likely to find a person while turning slowly though. In addition, there is additional tuning movement which rotates Sparky back right by a bit as there tends to be some delay
                between the face recognition and when Sparky&rsquo;s camera has already passed the target. Finally, after traveling to the requested person, Sparky&rsquo;s search will come to a close after the desired target has reached an
                experimentally determined threshold value for proximity based on the amount of screen space the target person&#39;s head takes up in the captured video.
            </span>
        </p>      
	      
	      
      </div>






    <hr id='results'>

      <div style="text-align:center;">
              <h2>Results</h2>
              <p style="text-align: left;padding: 0px 30px;">
                <p class="c6">
                  <span class="c7">
                      Overall Sparky performs spectacularly in day to day testing. Sparky is able to detect voices from good ranges away and responds to anyone&rsquo;s voice as long as they follow the pre-trained word models. Even over Zoom,
                      Sparky is likely to respond. As for motor control, Sparky is able to respond to movement commands consistently and also make his way to the desired target a large part of the time. While not perfect or as fast as we had
                      initially tested with, the tradeoff for very accurate face recognition seems to be a worthwhile enough limitation on his tracking ability and video processing. While we did not follow the goals we initially set out on to
                      their entirety, we find that the end result is very satisfying nonetheless. Some of the initial project ideas included, using an Alexa AI to handle voice processing as well as the potential for Sparky to deliver objects to
                      our targets as a bonus feature. Instead, we turned towards having Sparky do physical tricks with his wheels as well as being able to save photos and add decorations to peoples faces similar to AR Snapchat features.
                  </span>
              </p>
              <p class="c1"><span class="c7"></span></p>  
              </p>
      </div>

      <hr id='conclusion'>

    <div style="text-align:center;">
      <h2>Conclusion</h2>
      <p style="text-align: left;padding: 0px 30px;">
        <p class="c6">
          <span class="c7">
              We were able to create a fully autonomous robot that is controlled through voice and performs facial recognition for both of the creators as well as the face detection for other persons. All of the processing of voice and
              facial recognition is done on the Raspberry Pi. The system as a whole ended up being what we set out to do it includes the use of a camera, a microphone, DC motors with motor controller, and TFT display. The end result
              manifests itself as an interesting robot that behaves much like a pet, your very own desk companion SPARKY!
          </span>
      </p>
      <p class="c1"><span class="c7"></span></p>
      <p class="c6">
          <span class="c7">
              Our biggest challenges we faced when trying to implement the whole system have to be buffer size and the difference in hardware. The buffer size really infringed in the implementation of the project because we really wanted
              to be able to run both facial recognition and voice together but the size of the buffer did not allow for that when running facial recognition the voice input to the microphone will not be read into the buffer due to
              overflow. Our way around it was to first turn off the exception caused by the buffer overflow and then after put timers in place when Sparky will go into facial recognition mode. Turning off the overflow exception does
              diminish to some extent the voice recognition because missed information is now more common. The effect can also be seen with the facial recognition side in the frame rate and pixel density of the video. A larger buffer
              would&rsquo;ve allowed us to process information a lot more smoothly.
          </span>
      </p>
      <p class="c1"><span class="c7"></span></p>
      <p class="c6">
          <span class="c7">
              Since we built two separate Sparky systems the differences between dc motors, batteries, and driving terrain made us make adjustments to our implementation since many times going from one robot to another we noticed the
              discrepancy in the movement of sparky. In the case of search mode using facial recognition we opted for making slower and more jittery movements when spinning around looking for faces by stopping before making the next
              advancement. This was after attempting to use LBPH face recognition which we deemed to be too inaccurate.
          </span>
      </p>
      <h1 class="c42" id="h.a4bs2kbkhmsb"><span class="c41">Future Work</span></h1>
      <p class="c6">
          <span class="c7">
              We are very happy with the features we were able to give Sparky, but as always with more time we could have added much more to our friend. Looking into the possibilities of what could be next we could implement sensors that
              help Sparky navigate the desk better and keep him on top of it. The next evolution of Sparky should definitely involve a helper arm that can reach for objects around the table as well as an actuated stereo camera for better
              vision and depth sensing. Another way to help improve Sparky&rsquo;s performance could be by parallelizing the code using threads, scheduling based on priority and conditional variables to help with the heavy tool that voice
              and face recognition together put on the processor. Lastly, the mechanical design of Sparky could be refined to better handle the weight of the batteries and Raspberry PI
          </span>
      </p>
      </p>
    </div>

    <div class="row" style="text-align:center;">
          <h2>Work Distribution</h2>
          <p class="c6"><span class="c7">Overall the work for this project was pretty evenly distributed. We worked on the overall design and code development together as well as the final report.</span></p>
          <div style="text-align:center;">
              <img class="img-rounded" class="img-rounded" src="pics/group.jpg" alt="Generic placeholder image" style="width:80%;">
              <h4>Project group picture</h4>
          </div>
          <div class="col-md-6" style="font-size:16px">
              <img class="img-rounded" src="pics/a.png" alt="Generic placeholder image" width="240" height="240">
              <h3>John Ly</h3>
              <p class="lead">jtl222@cornell.edu</p>
              <p> Cornell Electrical and Computer Engineer 2021
              <p class="col-md-6 text-center"> Responsible for face detection and recognition, motor control functions, face tracking and pursuit mode, party mode effects, and video editing
          </div>
          <div class="col-md-6" style="font-size:16px">
              <img class="img-rounded" src="pics/b.png" alt="Generic placeholder image" width="240" height="240">
              <h3>Carlos Gutierrez</h3>
              <p class="lead">cag334@cornell.edu</p>
              <p>Cornell Electrical and Computer Engineer 2021
              <p class="col-md-6 text-center">Voice detection and processing, voice movement modes, tricks mode, piTFT image display
          </div>
      </div>

      <h1 class="c42" id="h.l3trz6x9tn96"><span class="c41">Team</span></h1>
      <p class="c6"><span> - Cornell Electrical and Computer Engineer 2021&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;jtl222@cornell.edu</span></p>
      <p class="c1"><span class="c7"></span></p>
      <p class="c6"><span class="c7"> - Cornell Electrical and Computer Engineer 2021&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cag334@cornell.edu</span></p>
      <p class="c1"><span class="c7"></span></p>
      <h1 class="c42" id="h.8bv9wvg0fgwd"><span>Appendix</span></h1>
      <h2 class="c9" id="h.wgnzbc5cjtjh"><span class="c4">Bill of Materials&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></h2>
      <a id="t.41f96062d17741918658df300cbb487cfa09eabb"></a><a id="t.3"></a>
      <table class="c17">
          <tbody>
              <tr class="c30">
                  <td class="c45" colspan="1" rowspan="1">
                      <p class="c24"><span class="c4">Product</span></p>
                  </td>
                  <td class="c62" colspan="1" rowspan="1">
                      <p class="c24"><span class="c4">Cost</span></p>
                  </td>
                  <td class="c26" colspan="1" rowspan="1">
                      <p class="c24"><span class="c4">Provided?</span></p>
                  </td>
              </tr>
              <tr class="c30">
                  <td class="c45" colspan="1" rowspan="1">
                      <p class="c24"><span class="c4">Raspberry Pi 4 2GB</span></p>
                  </td>
                  <td class="c62" colspan="1" rowspan="1">
                      <p class="c24"><span class="c4">$35.00</span></p>
                  </td>
                  <td class="c26" colspan="1" rowspan="1">
                      <p class="c24"><span class="c4">Yes</span></p>
                  </td>
              </tr>
              <tr class="c30">
                  <td class="c45" colspan="1" rowspan="1">
                      <p class="c24"><span class="c4">Adafruit piTFT 2.8&rdquo;</span></p>
                  </td>
                  <td class="c62" colspan="1" rowspan="1">
                      <p class="c24"><span class="c4">$35.00</span></p>
                  </td>
                  <td class="c26" colspan="1" rowspan="1">
                      <p class="c24"><span class="c4">Yes</span></p>
                  </td>
              </tr>
              <tr class="c30">
                  <td class="c45" colspan="1" rowspan="1">
                      <p class="c24"><span class="c4">2 DC motors &amp; Chassis </span></p>
                  </td>
                  <td class="c62" colspan="1" rowspan="1">
                      <p class="c24"><span class="c4">$13.99</span></p>
                  </td>
                  <td class="c26" colspan="1" rowspan="1">
                      <p class="c24"><span class="c4">Yes</span></p>
                  </td>
              </tr>
              <tr class="c30">
                  <td class="c45" colspan="1" rowspan="1">
                      <p class="c24"><span class="c4">SparkFun Motor Driver - Dual TB6612FNG (1A)</span></p>
                  </td>
                  <td class="c62" colspan="1" rowspan="1">
                      <p class="c24"><span class="c4">$4.96</span></p>
                  </td>
                  <td class="c26" colspan="1" rowspan="1">
                      <p class="c24"><span class="c4">Yes</span></p>
                  </td>
              </tr>
              <tr class="c30">
                  <td class="c45" colspan="1" rowspan="1">
                      <p class="c24"><span class="c4">Raspberry Pi Camera Module v2</span></p>
                      <sup><a href="#cmnt11" id="cmnt_ref11">[k]</a></sup><sup><a href="#cmnt12" id="cmnt_ref12">[l]</a></sup>
                  </td>
                  <td class="c62" colspan="1" rowspan="1">
                      <p class="c24"><span class="c4">$23.98</span></p>
                  </td>
                  <td class="c26" colspan="1" rowspan="1">
                      <p class="c24"><span class="c4">Yes (Requested)</span></p>
                  </td>
              </tr>
              <tr class="c30">
                  <td class="c45" colspan="1" rowspan="1">
                      <p class="c24"><span class="c4">USB microphone</span></p>
                  </td>
                  <td class="c62" colspan="1" rowspan="1">
                      <p class="c24"><span class="c4">$10.00</span></p>
                  </td>
                  <td class="c26" colspan="1" rowspan="1">
                      <p class="c24"><span class="c4">No</span></p>
                  </td>
              </tr>
          </tbody>
      </table>
      <p class="c1"><span class="c7"></span></p>



    <hr>
      <div style="font-size:18px">
          <h2>Parts List</h2>
          <ul>
              <li>Raspberry Pi $35.00</li>
              <li>Raspberry Pi Camera V2 $25.00</li>
              <a href="https://www.adafruit.com/product/1463"><li>NeoPixel Ring - $9.95</li></a>
              <li>LEDs, Resistors and Wires - Provided in lab</li>
          </ul>
          <h3>Total: $69.95</h3>
      </div>
      <hr>
      <div style="font-size:18px">
          <h2>References</h2>
          <a href="https://picamera.readthedocs.io/">PiCamera Document</a><br>
          <a href="http://www.micropik.com/PDF/SG90Servo.pdf">Tower Pro Servo Datasheet</a><br>
          <a href="http://getbootstrap.com/">Bootstrap</a><br>
          <a href="http://abyz.co.uk/rpi/pigpio/">Pigpio Library</a><br>
          <a href="https://sourceforge.net/p/raspberry-gpio-python/wiki/Home/">R-Pi GPIO Document</a><br>

      </div>

    <hr>

      <div class="row">
              <h2>Code Appendix</h2>
              <pre><code>
// Hello World.c
int main(){
  printf("Hello World.\n");
}
              </code></pre>
      </div>

    </div><!-- /.container -->




    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
    <script src="dist/js/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <!-- <script src="../../assets/js/ie10-viewport-bug-workaround.js"></script> -->
  </body>
</html>

<!-- Copy and paste the converted output. -->

<!-----
NEW: Check the "Suppress top comment" option to remove this info from the output.

Conversion time: 7.514 seconds.


Using this HTML file:

1. Paste this output into your source file.
2. See the notes and action items below regarding this conversion run.
3. Check the rendered output (headings, lists, code blocks, tables) for proper
   formatting and use a linkchecker before you publish this page.

Conversion notes:

* Docs to Markdown version 1.0β29
* Sun Dec 20 2020 00:23:04 GMT-0800 (PST)
* Source doc: Sparky - Robot Companion
* Tables are currently converted to HTML tables.
* This document has images: check for >>>>>  gd2md-html alert:  inline image link in generated source and store images to your server. NOTE: Images in exported zip file from Google Docs may not appear in  the same order as they do in your doc. Please check the images!


WARNING:
You have 8 H1 headings. You may want to use the "H1 -> H2" option to demote all headings by one level.

----->


<p style="color: red; font-weight: bold">>>>>>  gd2md-html alert:  ERRORs: 0; WARNINGs: 1; ALERTS: 10.</p>
<ul style="color: red; font-weight: bold"><li>See top comment block for details on ERRORs and WARNINGs. <li>In the converted Markdown or HTML, search for inline alerts that start with >>>>>  gd2md-html alert:  for specific instances that need correction.</ul>

<p style="color: red; font-weight: bold">Links to alert messages:</p><a href="#gdcalert1">alert1</a>
<a href="#gdcalert2">alert2</a>
<a href="#gdcalert3">alert3</a>
<a href="#gdcalert4">alert4</a>
<a href="#gdcalert5">alert5</a>
<a href="#gdcalert6">alert6</a>
<a href="#gdcalert7">alert7</a>
<a href="#gdcalert8">alert8</a>
<a href="#gdcalert9">alert9</a>
<a href="#gdcalert10">alert10</a>

<p style="color: red; font-weight: bold">>>>>> PLEASE check and correct alert issues and delete this message and the inline alerts.<hr></p>


<p>
  
</p>
<h1>Sparky - Robot Companion</h1>


<p>Monday Section
</p>
<p>December 17, 2020
</p>
<p>
ECE 5725 Final Project
</p>
<p>By: John Ly (jtl22) and Carlos Gutierrez (cag334)
</p>
<p>
<a href="https://drive.google.com/file/d/1hK0YGdr7ms5SGP1iTrqiTuEJ5RNxXEqx/view?usp=sharing">Showcase Video</a>
</p>
<p>
<a href="https://www.youtube.com/watch?v=yTBZ8xb8o1w&ab_channel=CarlosGutierrez">https://www.youtube.com/watch?v=yTBZ8xb8o1w&ab_channel=CarlosGutierrez</a>
</p>
<h1>Project Objective</h1>


<p>
The goal for our ECE 5725: Design with Embedded Operating Systems final project was to create a smart mobile robot which uniquely combined OpenCV facial recognition alongside voice commands to create a fun robot capable of identifying faces and traveling to them upon voice requests.
</p>
<h1>Introduction</h1>


<p>
Our final project is named Sparky. Sparky is an intelligent pet-like robot which listens to its owner and is able to accomplish unique tricks in some ways similar to a real live trained dog. The basis for this project is the Raspberry Pi 4 system combined with a piTFT display, a Raspberry Pi Camera, a microphone, and a motorized chassis. Using both <a href="https://docs.opencv.org/master/index.html">OpenCV</a> and the Python <a href="https://github.com/ageitgey/face_recognition">face_recognition</a> library, we are able to display on-screen video feedback of our camera signal with additional image processing to detect faces. A connected microphone allows Sparky to actively record sounds and listen for key words to act upon before doing a desired action, to do so Sparky uses the <a href="https://github.com/Picovoice/picovoice">picovoice</a> library along with custom created files using <a href="https://picovoice.ai/docs/quick-start/console-rhino/">picovoice console</a>. Lastly, the Raspberry Pi 4 is seated up above an acrylic frame in addition to a rechargeable battery pack, 4 AA batteries for our two DC motors, the motor controller, and our two wheels for locomotion.
</p>
<p>


<p id="gdcalert1" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image1.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert2">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image1.png" width="" alt="alt_text" title="image_tooltip">


<p id="gdcalert2" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image2.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert3">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image2.png" width="" alt="alt_text" title="image_tooltip">

</p>
<p>
Figure 1: Sparky Build - John’s on left (mic not shown), Carlos’s on right 
</p>
<h3>General Features</h3>


<p>
The key programmed features are as follows:
</p>
<p>
Voice Commands:
</p>
<p>
At any moment while Sparky is sitting still and not performing any of the tasks the user can call Sparky by using his name and then after providing the correct structure sentence with the intentions that the user wants Sparky to perform. Once Sparky detects his name an LED will light up (similar to Alexa) signaling that whatever is said next will be processed as the intentions (voice commands) of the user and determine if what is said is understandable. The LED light will be on for 5 seconds and then turn off, during the time the LED light is on the user can speak to Sparky, if the light is to turn off the hotword Sparky will need to be said again in order to speak to Sparky. 
</p>
<ul>

<li>Sparky (hot word for starting activation)

<li>Basic Movement 
<ul>
 
<li>Go/Move/do a - (1)Forward, (2)Back, (3)Left, (4)Right, (5) Spin
</li> 
</ul>

<li>Search 
<ul>
 
<li>Find / Look for - (1)John, (2)Carlos
</li> 
</ul>

<li>Tricks 
<ul>
 
<li>Party Mode/Time
 
<li>Break Dance - (1)Hit It, (2)Now
</li> 
</ul>
</li> 
</ul>
<p>
Face Identification:
</p>
<ul>

<li>Party Mode: most easily visible from party mode, Sparky will activate a 10s photo timer and additionally animate a party hat on any visible faces in front of Sparky. For recognized persons, they are given a proper party hat, while unknown faces are given a simple yellow triangle hat. The photo is then snapshotted and printed at the bottom left corner of our piTFT. The files are additionally saved to be opened later if desired.
</li>
</ul>
<p>
 

<p id="gdcalert3" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image3.jpg). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert4">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image3.jpg" width="" alt="alt_text" title="image_tooltip">


<p id="gdcalert4" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image4.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert5">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image4.png" width="" alt="alt_text" title="image_tooltip">

</p>
<ul>

<li>Search Mode: upon activation, Sparky will do some quick movements in search of the requested person, either Carlos or John. After doing some in place rotations, upon recognition, Sparky will then stop and then progressively drive towards the target until it is decided that the person requested is close enough to Sparky.
</li>
</ul>
<p>
Untethered Operation:
</p>
<ul>

<li>Sparky works independently of an internet connection and a wall-powered energy source. Sparky carries an on-board lithium rechargeable battery to power the Raspberry Pi and 4 AA batteries to deal with motor power.
</li>
</ul>
<p>
Is a Robot Pet:
</p>
<ul>

<li>Cuteness is always a bonus point.
</li>
</ul>
<h1>Design and Testing</h1>


<h2>Build and Construction</h2>


<p>


<p id="gdcalert5" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image5.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert6">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image5.png" width="" alt="alt_text" title="image_tooltip">

</p>
<p>
Figure 2: Sparky Wiring and Circuit Design
</p>
<p>
The assembly of our robo pet builds off of a previous lab, Lab 3, reusing the same chassis, motors, motor controller, and battery design. The major additions are the camera, the microphone, and additional construction materials for mounting.
</p>
<p>
This project uses a Raspberry Pi 4 2GB with PiTFT Display on top. Connected to the display is a breakout cable which is inserted into a full sized breadboard. Here on this breadboard, we have wiring for our motor controller as well as our two DC motors, and Sparky’s notification LED. These objects are all attached to a thin acrylic plate with mounting holes. While our overall construction is not ideal in terms of sturdiness, it is perfectly suitable for prototyping our design. Underneath and towards the back of our chassis is a ~2200mAh lithium ion battery responsible for directly powering our Raspberry Pi when Sparky is set to work untethered. Towards the front of the chassis is a 6V AA battery holder used to power our motors. Our motor controller is the Sparkfun TB6612FNG dual-channel motor controller as shown below. It gives us control of both independent motor operation as well as duty cycle to assist with making turns and changing speeds.
</p>
<p>
<em>PWMA 	    → 	GPIO 26		PWMB 	    → 	GPIO 16</em>
</p>
<p>
<em>INA1 	    →  	GPIO 4			INB1 	    →  	GPIO 20</em>
</p>
<p>
<em>INA2	    →  	GPIO 5			INB2 	    →  	GPIO 21</em>
</p>
<p>


<p id="gdcalert6" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image6.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert7">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image6.png" width="" alt="alt_text" title="image_tooltip">

</p>
<p>
Sparkfun TB6612FNG dual-channel motor controller
</p>
<p>
Shown below is the underside with our battery placement, motors, and wheels all easily visible. 
</p>
<p>


<p id="gdcalert7" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image7.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert8">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image7.png" width="" alt="alt_text" title="image_tooltip">

</p>
<p>
Figure 4: Underside of Sparky
</p>
<p>
As seen in <strong>Figure 1</strong>, the Raspberry Pi V2 Camera Module is placed directly facing forward and as high as possible upon the robot to get an angle closer to head level for face detection and recognition. Due to the constraints of our build, Sparky performs most optimally on a table surface where one’s face is most visible from the camera’s perspective due to the height. In a more advanced build there would be more room for an actuated camera with an additional degree of freedom to pivot and look around during operation. Our cameras are stood up onto our robot primarily using cardboard/wood splints. What is not displayed here is that Sparky additionally uses a USB connected mic for receiving audio. We individually found our own mics to use, but any small and lightweight omnidirectional microphone is ideal here. John substituted in a Blue Snowball iCE Microphone with a long wire while Carlos used a small headset and strapped it around Sparky.
</p>
<h2>Voice recognition</h2>


<h3>Microphone Input:</h3>


<p>
With any USB microphone connected to the Py we use the pyaudio() library to establish an audio stream from the microphone by setting variable <code>pa = pyaudio.PyAudio(). </code>Next we open the audio stream<code> pa.open()</code> and set the sample rate, channels, format, and size of the buffer inside this variable (configuration based on the pyadudio library). The values used for the <code>rate</code>,<code> format</code> and <code>frames_per_buffer</code> are taken from the picovoice library because we will later process the audio stream using the picovoice library and there must be congruence between the audio format and the necessary requirements of the picovoice library. The audio_stream variable setup is shown below. 
</p>



<pre class="prettyprint">audio_stream = pa.open(
	rate=_picovoice.sample_rate,
	channels=1,
	format=pyaudio.paInt16,
	input=True,
	frames_per_buffer=_picovoice.frame_length)</pre>


<h3><strong>Using Picovoice():</strong> </h3>


<p>
The picovoice engine accepts 16-bit linearly-encoded PCM and operates on a single-channel. The audio is processed in consecutive frames. Picovoice is able to understand wake words (hotword) and after the recognition of the wake word it detects the intentions of the user using inference. Both the wake word and the inference are customizable and can be created using the online picovoice console, after the desired phrases are created and trained online we can download the trained files that pertain to the wake word and inferences. The wake word is .ppn type file and the inference is a .rhn type file. The path to these files are placed inside variable <code>_picovoice</code> along with other configurations seen below.
</p>



<pre class="prettyprint">_picovoice=Picovoice(
	keyword_path=_keyword_path,
	wake_word_callback=wake_word_callback,
	context_path=_context_path,
	inference_callback=inference_callback,
	porcupine_library_path=None,
	porcupine_model_path=None,
	rhino_sensitivity=0.2, 
	rhino_library_path=None,
	rhino_model_path=None,
	porcupine_sensitivity=1)</pre>


<p>
Variables<code> wake_word_callback</code> and<code> inferecence_callback</code> are assigned to a respective function that handles the signal of such an event occurring. Variables <code>rhino_sensitivity</code> and<code> porcupine_sensitivy</code> are assigned a value from [0-1] where 1 is the most sensitive value for the library to react to inferences and to the wake word respectively. We set the hot word sensitivity to the 1 to make sure a call to Sparky will be very easily recognized. The value set for the inference is low to make sure that the spoken commands are correctly ottered and so that the library is not set off by incorrectly uttered commands  that are not part of Sparky’s understanding. 
</p>
<p>
We opted for our audio to be constantly running and processing the information gathered from the microphone, therefore at the beginning of the while loop we set variable<code> pcm = audio_stream.read(_picovoice.frame_length,exception_on_overflow=False)</code> in order to read the microphone, on the next line we edit the variable <code>pcm = struct.unpack_from("h" * _picovoice.frame_length, pcm) </code>to unpack the information from the audio into a format picovoice expects and finally we process the data<code> _picovoice.process(pcm)</code>.
</p>
<p>
<em>def wake_word_callback():</em>
</p>
<p>
This function is accessed when the word “Sparky” is recognized by picovoice engine. Here we set all mode flags to false, light up the LED connected to GPIO pin 13, set flag intention.led_on to True which will allow us to turn off the LED after 5 seconds, we also stop the motors of the robot. The main purpose behind this is to give the sense that when Sparky is called he stops what he is doing and listens to the next instructions. 
</p>
<p>
<em>def inference_callback():</em>
</p>
<p>
After the wake word is recognized and the inferences spoken are understood this function accessed in here we enable the flags that set off the modes inside the while based on the values of<code> inference.intent</code> which can be either ‘move’ or search’ strings. Variable <code>inference. slots[] </code>is a dictionary type and contains the value of slot under the uttered intent. Along with enabling the correct flag we write the value of the action Sparky is taking into variable <code>intention.word</code> as a string ( used later on to write on the TFT display). We initiate variable <code>timerStart= time.time()</code> to be used in modes where a time limit is implemented, we read this value to know when the mode began. 
</p>
<h2>Face Detection and Recognition</h2>


<h3>Face Detection Algorithm</h3>


<p>
Overall while many face detection and recognition libraries, choosing an appropriate method for our device had some challenges. Initially, we developed Sparky using the Local Binary Patterns Histograms (LBPH) algorithm available through the OpenCV library. More information about how this method works can be found <a href="https://towardsdatascience.com/face-recognition-how-lbph-works-90ec258c3d6b">here</a>, but in summary the algorithm takes in grayscale images and uses thresholding to turn the pixel array into binary data. From here a histogram of the data is created and based on new incoming images. Those images are treated similarly and compared across various histogram values. Based on some differences between the values, a confidence value can be calculated to determine a person’s face. Initially this method seemed to work fine, properly identifying and labeling John’s face, but after further testing, we found that while the algorithm works okay for already pretrained images, it had some biases towards already trained faces and resulted in lots of errors especially with faces that were supposed to be unknown to the system. 
</p>
<p>
As a result, towards the later half of the project we pivoted towards using the significantly more accurate face_recognition library linked <a href="https://github.com/ageitgey/face_recognition">here</a>. According to the documentation, this library was trained using the <a href="http://dlib.net/">Dlib C++</a> library along with deep learning methods. The result here is that key facial features are determined primarily through color gradient changes within a face. These results are recorded in a histogram of oriented gradients (HOG) which is then encoded for usage with our facial recognition model. With these gradients it is possible to discern key landmark features such as the eye shape, eyebrows, nose shape, mouth, and jawline. Additional details on the deep learning algorithm used can be found <a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78">here</a>.
</p>
<p>
In our pivot from using the LBPH model method provided in OpenCV to the face_recogniton library written by ageitgey, there were a few major resulting changes. Firstly, face recognition was significantly improved and with very high accuracy. The major tradeoff though was that the necessary computing power was much more significant and resulted in our smooth video feed losing many frames. A few factors are the cause for this, but some potential solutions for improving computation times would be having a processor better suited for calculated parallel computations and with better single core performance. On a standard desktop, this can be achieved through using Nvidia’s CUDA cores or a high performance CPU. One potential other issue is that our compilation of dlib using pip3 install may have some strange performance errors as opposed to compiling the library on board our Raspberry Pi. This has not been tested though as this potential solution was only recently found by us.
</p>
<h3>Face Recognition Training Data</h3>


<p>
As Sparky is currently, he only can recognize pretrained faces. It would very much be possible to have active face recognition work with new faces during operation but we’ll leave that for a possible future implementation. Using this <a href="https://www.tomshardware.com/how-to/raspberry-pi-facial-recognition">guide</a> from Caroline Dunn, we were able to make a face recognition system that ran using active video data instead of just identifying still images as provided by the face_recognition library. To first train our encodings for face recognition, we start by taking a collection of photos of primarily one's face while rotated to different angles. For John, he additionally took photos with and without glasses. We used about 20-30 images but the face_detection library is supposedly very capable with even just 10 images. With our input data of identifying names and images, we first take our image data captured in BGR format and convert it to RGB before using the face recognition library to determine face positions as well as create encodings in HOG model format.
</p>



<pre class="prettyprint">    boxes = face_recognition.face_locations(rgb, model="hog")
    encodings = face_recognition.face_encodings(rgb, boxes)
    for encoding in encodings:
        knownEncodings.append(encoding)
        knownNames.append(name)
</pre>


<p>
Lastly, using the Python pickle library allows us to save our data into a serialized format.
</p>



<pre class="prettyprint">data = {"encodings": knownEncodings, "names": knownNames}
f = open("encodings.pickle", "wb")
f.write(pickle.dumps(data))
f.close()
</pre>


<h3>Face Recognition Implementation</h3>


<p>
For both face recognition algorithms described, our system first required face detection using OpenCV’s Haar Feature-based Cascade Classifier model which is able to quickly detect objects, but in our case faces. The difference between face detection and face recognition is that recognition is more specific and allows for us to distinguish between faces whereas face detection only provides us with the ability to determine if a human face is present. The model is available pre-trained but is generally created using lots of positive (faces) and negative (no faces) images. From there, the differences allow for key features to be extracted and to create a model. The return from this face detection when using <code>detector.detectMultiScale() </code>where detector is a <code>CascadeClassifier</code> is a set of 4 values describing the pixel values of a box surrounding the detected person’s face from within an image. The format of this is (x,y,w,h) where x and y describe the starting position of the box from the top left corner while w and h describe the pixel width and height of the box respectively. While this classifier only uses grayscale images, our face recognition algorithm uses RGB format images thus we must create both forms of images. For those unfamiliar, it is also important to note that in pixel arrays (0,0) starts from the top left corner.
</p>



<pre class="prettyprint">    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # simple face detection
    faces = detector.detectMultiScale( img_gray, scaleFactor = 1.3, 
minNeighbors = 5, minSize = (int(minW), int(minH)), 
flags = cv2.CASCADE_SCALE_IMAGE
    )
</pre>


<p>
Using this data we are able to zoom in on the data that we want to process for face recognition and individually compare each boxed face to our encoded HOG dataset for identification. While the function <code>detect_faces_quick() </code>primarily just uses the above lines of code for face detection, the function <code>identify_faces() </code>is used for proper face recognition and deals with the comparisons as previously mentioned. Using new camera data and previously trained encodings created from the face_recogntion library, we can make our identifications and label the faces accordingly. 
</p>
<p>
In addition to the OpenCV library being responsible for opening up our camera and reading the incoming signal, we also used it for drawing and labeling on top of our images. The library includes the ability to draw shapes and write text as desired at different pixel coordinates. Some of that can be seen below. In the case that we cannot identify a person, the written name is defaulted to “Unknown” and they are labeled as such.
</p>



<pre class="prettyprint">  cv2.ellipse(img, (center[0], center[1]), (w//2, h//2), 0, 0, 360, (255, 0, 255), 2)   
  cv2.putText(img, str(name), (leftX-15,bottomY+30), font, 1.2, (255,255,255), 2)
</pre>


<h3>Move mode:</h3>


<p>
When <code>instruction.v_direction</code> is True, Sparky’s motors move in the direction specified by the user that direction is read from variable instruction.word. For each of the directions there is a call to function <code>movement_3sec("direction",speed, speed)</code> which will make the motors move in the “direction” uttered. Inside the function<code> movement_3sec()</code> a timer is implemented for directions ('forward', 'backward', 'right', and  'left') of half a second implemented to make Sparky stop after the timer is over. Using pygame we also update the TFT display message to display what Sparky is doing. 
</p>



<pre class="prettyprint">if instruction.v_direction:
	if instruction.word == 'forward':
				movement_3sec('forward',90,90)
	if instruction.word == 'back':
				movement_3sec('backward',90,90)
	if instruction.word == 'right':
				movement_3sec('right',90,90)
	if instruction.word == 'left':
				movement_3sec('left',90,90)
	if instruction.word == 'spin':
				tw.drive("left",90,90)
			
	screen.fill(BLACK, textAreaRect)
	text = fontLg.render(f'Moving {instruction.word}', True, WHITE, BLACK)
	textRect = text.get_rect(center=(0.5*dispW, 0.75*dispH))
	screen.fill(BLACK, textRect.inflate(240,0))
            screen.blit(text,textRect)
            pygame.display.update(textRect)</pre>


<h3>Search Mode:</h3>


<p>
When variable <code>instruction.v_search</code> is true we can tell Sparky to begin the process of finding the user specified. First the text on theTFT is updated to show the change in mode, Sparky displays ‘Looking for ‘user’...’. A timer condition of 20 seconds is set so that Sparky only performs the task of searching for 20 seconds. Inside this condition there is a call to the function <code>foundFace = find_faces(instruction.word, img)</code>. If the function returns True, that means the target user was found in that direction, then we stop running the<code> find_faces</code> function and we proceed to running<strong> </strong>the <code>detect_faces_quick</code> function. Our return value,<code> facePresent = detect_faces_quick(img)</code>will be true if there is a singular detectable face still in the view of the camera. If facePresent is true we perform <code>pursue_target(True)</code> which will guide the robot closer towards the face based on the location of the face on the image from the camera. If <code>facePresent</code> is false then we run the function <code>identify_faces()</code> which determines if the user requested is found in the field of view of the camera, this function then returns a value for <code>target</code> and then again we perform<code> pursue_target</code> if<code> target=True.</code>In the case of<code> target= False</code> we increment variable<code> misses</code>, after 10 consecutive misses (<code>misses >10</code>) we then try again and run the <code>find_faces</code> function. 
</p>
<p>
The main reason for dividing the face recognition steps like this is to make an adjustment between running a very heavy face recognition process, which provides the best results on identifying the user, and being able to pursuing the target in the correct direction. If the heavy face recognition function is always running it has an enormous effect on the framerate of the camera which then translates to erroneous movements of the robot because we are moving faster than what our image processing can return. This causes a delay on the reaction of the motors and a bad performance by Sparky. We opted to implement the search algorithm this way so that after finding our target our driving motion towards the face consists of using face detection only for approximately 95% of the time (light on the processor) and then we use proper face recognition for the remaining other 5% of the time (heavy on the processor). This way we make sure that Sparky makes faster motor adjustment decisions when moving towards the target and also that he recognizes the face with high precision. Once Sparky reaches the target user, the flag<code> stopCondition</code> is set and we exit the search mode (set<code> instruction.v_search =False</code>) and set all variables back to their starting value, we also update the TFT display to notify that the user was found. While Sparky is trying to find the user, a counter is displayed onto the screen showing how much time has elapsed since the search began, and as previously stated once the count reaches its limit of 20 seconds, Sparky stops the search.
</p>
<h3>Tricks Mode:</h3>


<p>
For Sparky to go into this mode the variabel<code> instruction.v_tricks</code> is set to true by the voice recognition. Inside this condition we look at variable<code> instruction.word</code> and determine if it's equal to ‘party’ or ‘break dance’, the two tricks that Sparky can do. 
</p>
<p>
If <code>instruction.word == 'party', </code>we then again update the tft display to show what Sparky is doing and we run function <code>_,_,img = identify_faces(None, img, mode = "party_time")</code> this is the same face detection function previously used by the search mode but since in this case the motors for Sparky are not running we take the privilege to run it without constraints. We pass mode= “party_time” to the function to tell it that it will not only do face recognition but will also draw on top of the user the trade-marked party hats. For users that Sparky knows (Carlos and John) the hat is special and for people that Sparky doesn’t know yet they will receive just an outlined triangle. This mode will run for 10 seconds and also display a 10 second counter on the screen at the end of the time Sparky exits the mode and waits for his next instructions. 
</p>
<p>


<p id="gdcalert8" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image8.gif). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert9">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image8.gif" width="" alt="alt_text" title="image_tooltip">


<p id="gdcalert9" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image9.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert10">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image9.png" width="" alt="alt_text" title="image_tooltip">

</p>
<p>
If <code>instruction.word == 'break dance'</code>, then TFT display is updated and a function call to move_breakdance() is made. This function moves the motors so that sparky alternates between moving right and left. Sparky looks like he is wiggling in place. 
</p>
<p>


<p id="gdcalert10" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image10.gif). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert11">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image10.gif" width="" alt="alt_text" title="image_tooltip">

</p>
<h2>Motor Control and Face Pursuit Mode</h2>


<h3>Motor Control</h3>


<p>
Sparky’s motor control largely comes from previously developed code for Lab 3 of ECE 5725. The code exists in two_wheel_mod.py and is responsible for initializing PWM signals for controlling the duty cycle of our motors. While the set duty cycle controls the turn speed, we can change the voltage sent to our control pins to also change the turn direction of our motors. With these functions combined, the file primarily includes a function <code>drive() </code>which makes it easier for making movement decisions with our robot. For example, it allows us to adjust for movement forward as well as individual motor spin speed. This is especially key to making micro adjustments to our travel path with Sparky when he is traveling forward. 
</p>
<h3>Face Pursuit</h3>


<p>
When locating a person from one of the three functions, <code>find_faces()</code>, <code>detect_faces_quick()</code>, or <code>identify_faces()</code> our robot Sparky will stop his motors and then determine an appropriate path for traveling to our desired target. Taking the center of our target as identified in our video feed, we can determine a relative offset from the center and therefore slow down the respective motors such that our robot will slowly turn towards our target while trying to maintain the target position at the center of Sparky’s vision. These conditions occur inside of <code>pursue_target()</code> and adjust our robot accordingly. We have this driving mode occur for a short period of time with our robot stuck inside a while loop, as we cannot process voice data during search mode anyways. When exiting, Sparky will then try to evaluate his position relative to the target. During this reevaluation, Sparky will then decide to do one of the three previously mentioned functions.
</p>
<p>
As detailed earlier in the Face Recognition Implementation section, Sparky will use <code>detect_faces_quick()</code>, to just follow the face in front of him. If there is more than one or less than one face visible, we will default to doing a fully intensive <code>identify_faces()</code> call which provides us details on which of the correct faces to pursue. Because identify_faces() is so intensive for our image processing, we use a randomly generated number to allow Sparky to check using identify_faces about 1% of the time inside of detect_faces_quick in the scenario where this one visible subject. While not an ideal way of truly making our search process, use identify_faces only about 1% of the time, we found the solution to be good enough. In the case that we ever got a wrong reading for face recognition, which is unlikely, or our robot has lost sight of the target, Sparky will count the amount of consecutive misses and eventually return to the  <code>find_faces()</code> function if we cannot find our target. Lastly, the <code>find_faces()</code> operates by primarily having Sparky pivot counterclockwise, while slowly polling for the target face using <code>identify_faces()</code>. To start though, upon a new search request, Sparky will do a quick pivot left and then right as a fun gesture, making him act like a robo pet. In the case that the desired person is found, Sparky will then exit and make his travel towards the target. Sparky is more likely to find a person while turning slowly though. In addition, there is additional tuning movement which rotates Sparky back right by a bit as there tends to be some delay between the face recognition and when Sparky’s camera has already passed the target. Finally, after traveling to the requested person, Sparky’s search will come to a close after the desired target has reached an experimentally determined threshold value for proximity based on the amount of screen space the target person's head takes up in the captured video.
</p>
<h1>Results</h1>


<p>
Overall Sparky performs spectacularly in day to day testing. Sparky is able to detect voices from good ranges away and responds to anyone’s voice as long as they follow the pre-trained word models. Even over Zoom, Sparky is likely to respond. As for motor control, Sparky is able to respond to movement commands consistently and also make his way to the desired target a large part of the time. While not perfect or as fast as we had initially tested with, the tradeoff for very accurate face recognition seems to be a worthwhile enough limitation on his tracking ability and video processing. While we did not follow the goals we initially set out on to their entirety, we find that the end result is very satisfying nonetheless. Some of the initial project ideas included, using an Alexa AI to handle voice processing as well as the potential for Sparky to deliver objects to our targets as a bonus feature. Instead, we turned towards having Sparky do physical tricks with his wheels as well as being able to save photos and add decorations to peoples faces similar to AR Snapchat features.
</p>
<h1>Conclusion</h1>


<p>
We were able to create a fully autonomous robot that is controlled through voice and performs facial recognition for both of the creators as well as the face detection for other persons. All of the processing of voice and facial recognition is done on the Raspberry Pi. The system as a whole ended up being what we set out to do it includes the use of a camera, a microphone, DC motors with motor controller, and TFT display. The end result manifests itself as an interesting robot that behaves much like a pet, your very own desk companion SPARKY!
</p>
<p>
Our biggest challenges we faced when trying to implement the whole system have to be buffer size and the difference in hardware. The buffer size really infringed in the implementation of the project because we really wanted to be able to run both facial recognition and voice together but the size of the buffer did not allow for that when running facial recognition the voice input to the microphone will not be read into the buffer due to overflow. Our way around it was to first turn off the exception caused by the buffer overflow and then after put timers in place when Sparky will go into facial recognition mode. Turning off the overflow exception does diminish to some extent the voice recognition because missed information is now more common. The effect can also be seen with the facial recognition side in the frame rate and pixel density of the video. A larger buffer would’ve allowed us to process information a lot more smoothly. 
</p>
<p>
Since we built two separate Sparky systems the differences between dc motors, batteries, and driving terrain made us make adjustments to our implementation since many times going from one robot to another we noticed the discrepancy in the movement of sparky. In the case of search mode using facial recognition we opted for making slower and more jittery movements when spinning around looking for faces by stopping before making the next advancement. This was after attempting to use LBPH face recognition which we deemed to be too inaccurate.
</p>
<h1>Future Work</h1>


<p>
We are very happy with the features we were able to give Sparky, but as always with more time we could have added much more to our friend. Looking into the possibilities of what could be next we could implement sensors that help Sparky navigate the desk better and keep him on top of it. The next evolution of Sparky should definitely involve a helper arm that can reach for objects around the table as well as an actuated stereo camera for better vision and depth sensing. Another way to help improve Sparky’s performance could be by parallelizing the code using threads, scheduling based on priority and conditional variables to help with the heavy tool that voice and face recognition together put on the processor. Lastly, the mechanical design of Sparky could be refined to better handle the weight of the batteries and Raspberry PI
</p>
<h1>Team</h1>


<p>
John Ly - Cornell Electrical and Computer Engineer 2021	jtl222@cornell.edu
</p>
<p>
Carlos Gutierrez - Cornell Electrical and Computer Engineer 2021	cag334@cornell.edu
</p>
<h1>Appendix</h1>


<h2>Bill of Materials	</h2>



<table>
  <tr>
   <td><strong>Product</strong>
   </td>
   <td><strong>Cost</strong>
   </td>
   <td><strong>Provided?</strong>
   </td>
  </tr>
  <tr>
   <td><strong>Raspberry Pi 4 2GB</strong>
   </td>
   <td><strong>$35.00</strong>
   </td>
   <td><strong>Yes</strong>
   </td>
  </tr>
  <tr>
   <td><strong>Adafruit piTFT 2.8”</strong>
   </td>
   <td><strong>$35.00</strong>
   </td>
   <td><strong>Yes</strong>
   </td>
  </tr>
  <tr>
   <td><strong>2 DC motors & Chassis </strong>
   </td>
   <td><strong>$13.99</strong>
   </td>
   <td><strong>Yes</strong>
   </td>
  </tr>
  <tr>
   <td><strong>SparkFun Motor Driver - Dual TB6612FNG (1A)</strong>
   </td>
   <td><strong>$4.96</strong>
   </td>
   <td><strong>Yes</strong>
   </td>
  </tr>
  <tr>
   <td><strong>Raspberry Pi Camera Module v2</strong>
   </td>
   <td><strong>$23.98</strong>
   </td>
   <td><strong>Yes (Requested)</strong>
   </td>
  </tr>
  <tr>
   <td><strong>USB microphone</strong>
   </td>
   <td><strong>$10.00</strong>
   </td>
   <td><strong>No</strong>
   </td>
  </tr>
</table>


<h2>References</h2>


<p>
Voice:
</p>
<p>
<a href="https://github.com/Picovoice/picovoice">https://github.com/Picovoice/picovoice</a>
</p>
<p>
<a href="https://pimylifeup.com/raspberry-pi-porcupine/">https://pimylifeup.com/raspberry-pi-porcupine/</a>
</p>
<p>
<a href="https://picovoice.ai/docs/api/picovoice-python/">https://picovoice.ai/docs/api/picovoice-python/</a>
</p>
<p>
<a href="https://pypi.org/project/pvporcupinedemo/">https://pypi.org/project/pvporcupinedemo/</a>
</p>
<p>
<a href="https://console.picovoice.ai/rhn/835553fe-4051-4d9e-8552-ed07a6e29d01#intent-move">https://console.picovoice.ai/rhn/835553fe-4051-4d9e-8552-ed07a6e29d01#intent-move</a>
</p>
<p>
<a href="https://stackoverflow.com/questions/10733903/pyaudio-input-overflowed">https://stackoverflow.com/questions/10733903/pyaudio-input-overflowed</a>
</p>
<p>
<a href="https://github.com/Picovoice/picovoice/blob/master/demo/python/picovoice_demo_mic.py">https://github.com/Picovoice/picovoice/blob/master/demo/python/picovoice_demo_mic.py</a>
</p>
<p>
Vision:
</p>
<p>
<a href="https://www.pyimagesearch.com/2018/09/24/opencv-face-recognition/">https://www.pyimagesearch.com/2018/09/24/opencv-face-recognition/</a>
</p>
<p>
<a href="https://projects.raspberrypi.org/en/projects/getting-started-with-picamera/6">https://projects.raspberrypi.org/en/projects/getting-started-with-picamera/6</a>
</p>
<p>
<a href="https://www.pyimagesearch.com/2015/03/30/accessing-the-raspberry-pi-camera-with-opencv-and-python/">https://www.pyimagesearch.com/2015/03/30/accessing-the-raspberry-pi-camera-with-opencv-and-python/</a>
</p>
<p>
<a href="https://docs.opencv.org/master/d6/d00/tutorial_py_root.html">https://docs.opencv.org/master/d6/d00/tutorial_py_root.html</a>
</p>
<p>
<a href="https://towardsdatascience.com/real-time-face-recognition-an-end-to-end-project-b738bb0f7348">https://towardsdatascience.com/real-time-face-recognition-an-end-to-end-project-b738bb0f7348</a>
</p>
<p>
<a href="https://towardsdatascience.com/face-recognition-how-lbph-works-90ec258c3d6b">https://towardsdatascience.com/face-recognition-how-lbph-works-90ec258c3d6b</a>
</p>
<p>
<a href="https://www.tomshardware.com/how-to/raspberry-pi-facial-recognition">https://www.tomshardware.com/how-to/raspberry-pi-facial-recognition</a>
</p>
<p>
<a href="https://github.com/ageitgey/face_recognition">https://github.com/ageitgey/face_recognition</a>
</p>
<h2>Work Distribution</h2>


<p>
Overall the work for this project was pretty evenly distributed. We worked on the overall design and code development together as well as the final report.
</p>
<ul>

<li>John Ly - Responsible for face detection and recognition, motor control functions, face tracking and pursuit mode, party mode effects, and video editing

<li>Carlos Gutierrez - Voice detection and processing, voice movement modes, tricks mode, piTFT image display
</li>
</ul>
<h2>Code</h2>


<p>
<a href="https://github.coecis.cornell.edu/jtl222/ECE-5725-OpenCV-Robot">https://github.coecis.cornell.edu/jtl222/ECE-5725-OpenCV-Robot</a>
</p>